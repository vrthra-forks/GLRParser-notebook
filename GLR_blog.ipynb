{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b02eb0e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Generalised LR (GLR)\n",
    "First introduced in 1965 by Knuth [1], LR is a bottom-up parsing technique that works by constructing an automaton, then traveling through it while consuming input one by one and maintaining a stack. A more detailed description about LR can be found [here](https://rahul.gopinath.org/post/2024/07/01/lr-parsing/).\n",
    "\n",
    "Creating a grammar in LR(1), or even LR(k), can be difficult. Ideally, you want your grammar to be intuitive, easily understood, and readable. This is important because you'll make mistakes or change your mind in the future, and modifying a large grammar while making sure that it is in LR(1) is a painful process. Luckily, a more powerful parser, that can handle all context-free grammar, called Generalised LR (GLR) is available. This post outlines the implementation details of two GLR variants (RNGLR and BRNGLR), both of which are presented in Economopoulos's PhD dissertation [2].\n",
    "### Preliminary\n",
    "This post assumes that the reader is already familiar with LR parsing and context-free grammar terminology (non-terminal, derivation, nullable,...).\n",
    "\n",
    "The parser implementation here uses the fuzzingbook format for input grammar. For example a grammar of the form $$\\begin{split} S &\\rightarrow A+A\\ |\\ A-A\\\\\n",
    "A &\\rightarrow a\\ |\\ b\\end{split}$$\n",
    "Is presented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b315dc0f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sample_grammar = {\n",
    "\t'<S>': [['<A>', '+', '<A>'],\n",
    "\t\t\t['<A>', '-', '<A>']],\n",
    "\t'<A>': [['a'], ['b']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b3c69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Table Generator\n",
    "This GLR implementation uses LR(1) as the base parse table, here we introduce all the needed components for an LR(1) parse table generator.\n",
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8812fa05-f5b1-4a2c-9c8b-e5c1472478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e83d14-1c42-4070-9e32-82f00d84d9b3",
   "metadata": {},
   "source": [
    "##### Types and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d2e333",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "Grammar = dict[str, list[list[str]]]\n",
    "# State in the LR automaton\n",
    "State = tuple[int, set[\"Item\"]]\n",
    "\n",
    "LOGGING = False\n",
    "\n",
    "def is_nt(k: str):\n",
    "    '''\n",
    "        Check if k is a non-terminal\n",
    "    '''\n",
    "    return (k[0], k[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6885c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Item class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50951b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class Item:\n",
    "    def __init__(self, lhs: str, rhs: list[str], dot: int, look_ahead: str):\n",
    "        '''\n",
    "            An item is a production of the form A -> a·b\n",
    "            \n",
    "            Args:\n",
    "                lhs: The left-hand side of the production (A)\n",
    "                rhs: The right-hand side of the production (a b)\n",
    "                dot: The position of the dot in the production (dot = 0 means A -> ·a b)\n",
    "                look_ahead: look ahead symbol\n",
    "        '''\n",
    "        self.lhs = lhs\n",
    "        self.rhs = rhs\n",
    "        self.dot = dot\n",
    "        self.look_ahead = look_ahead\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.lhs} -> {' '.join(self.rhs[:self.dot])} · {' '.join(self.rhs[self.dot:])}, {self.look_ahead}\"\n",
    "        # format: A -> a · b, look_ahead\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lhs == other.lhs and self.rhs == other.rhs and self.dot == other.dot and self.look_ahead == other.look_ahead\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.lhs, tuple(self.rhs), self.dot, self.look_ahead))\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        '''\n",
    "            For sorting\n",
    "        '''\n",
    "        return (self.lhs, self.rhs, self.dot, self.look_ahead) < \\\n",
    "               (other.lhs, other.rhs, other.dot, other.look_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c456c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### TableGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39a4a88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class TableGenerator:\n",
    "    def __init__(self, grammar: Grammar, start: str):\n",
    "        '''\n",
    "            This class is responsible for generating the right-nulled parse table.\n",
    "        '''\n",
    "\n",
    "        self.end_of_input = \"€\"\n",
    "        self.grammar = grammar\n",
    "        self.start = self.add_new_start(start)\n",
    "\n",
    "        self.rule_list = self.reformat_grammar(grammar)\n",
    "        self.nullable = TableGenerator.get_nullable(grammar)\n",
    "        self.first = self.init_FIRST(grammar)\n",
    "        self.follow = self.init_FOLLOW(grammar, self.start)\n",
    "\n",
    "        self.symbols = self.get_symbols(grammar)\n",
    "\n",
    "        self.sppf = SPPF(grammar)\n",
    "    \n",
    "    def get_symbols(self, grammar: Grammar):\n",
    "        '''\n",
    "            Return the list of all terminals, and non-terminals\n",
    "        '''\n",
    "        symbols: set[str] = set()\n",
    "        for nt in grammar:\n",
    "            for production in grammar[nt]:\n",
    "                for symbol in production:\n",
    "                    symbols.add(symbol)\n",
    "        \n",
    "        return symbols\n",
    "    \n",
    "    def add_new_start(self, start:str):\n",
    "        '''\n",
    "            Augment new start symbol <S'>\n",
    "        '''\n",
    "        new_start = ''.join([start[:-1], \"'\", start[-1:]])\n",
    "        new_production = [[start]]\n",
    "        self.grammar[new_start] = new_production\n",
    "\n",
    "        return new_start\n",
    "\n",
    "    def reformat_grammar(self, grammar: Grammar) -> list[tuple[str, list[str]]]:\n",
    "        '''\n",
    "            Transform grammar from a dictionary to a list of rules\n",
    "\n",
    "            args\n",
    "                grammar: the given grammar, in dictionary form\n",
    "            \n",
    "            return\n",
    "                A list of rules, a rule is a tuple with lhs and rhs \n",
    "        '''\n",
    "\n",
    "        rule_list = []\n",
    "\n",
    "        for nt in grammar.keys():\n",
    "            for production in grammar[nt]:\n",
    "                rule_list.append((nt, production))\n",
    "        return rule_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_nullable(grammar: Grammar) -> set[str]:\n",
    "        '''\n",
    "            Get nullable set, a non-terminal is called nullable if X can derive epsilon \n",
    "\n",
    "            return\n",
    "                The set of nullable non-terminals\n",
    "        '''\n",
    "        res: set[str] = set()\n",
    "        prev_size = -1\n",
    "        while (prev_size != len(res)):\n",
    "            prev_size = len(res)\n",
    "            for nt in grammar.keys():\n",
    "                for production in grammar[nt]:\n",
    "                    if (len(production) == 0):\n",
    "                        res.add(nt)\n",
    "                        continue\n",
    "                    \n",
    "                    if (all((is_nt(symbol) and symbol in res) for symbol in production)):\n",
    "                        res.add(nt)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def init_FIRST(self, grammar: Grammar) -> dict[str, set[str]]:\n",
    "        '''\n",
    "            Calculate the FIRST set for all symbols\n",
    "            FIRST(X) is the set of terminals that can start a string derived from X\n",
    "\n",
    "            return\n",
    "                A dictionary with all the non-terminal as keys and FIRST sets as values\n",
    "        '''\n",
    "        first: dict[str, set[str]] = {}\n",
    "        for nt in grammar:\n",
    "            first[nt] = set()\n",
    "            if nt in self.nullable:\n",
    "                first[nt].add(\"epsilon\")\n",
    "        \n",
    "        changed = True\n",
    "        while (changed):\n",
    "            changed = False\n",
    "            for lhs in grammar:\n",
    "                for rhs in grammar[lhs]:\n",
    "                    # Epsilon rule, already handled above\n",
    "                    if (len(rhs) == 0):\n",
    "                        continue\n",
    "\n",
    "                    for symbol in rhs:\n",
    "                        if (is_nt(symbol)):\n",
    "                            for char in first[symbol]:\n",
    "                                if char not in first[lhs]:\n",
    "                                    first[lhs].add(char)\n",
    "                                    changed = True\n",
    "                            \n",
    "                            if symbol not in self.nullable:\n",
    "                                break\n",
    "                        # Is terminal\n",
    "                        else:\n",
    "                            if symbol not in first[lhs]:\n",
    "                                first[lhs].add(symbol)\n",
    "                                changed = True\n",
    "                            break\n",
    "    \n",
    "        return first\n",
    "    \n",
    "    def calculate_FIRST(self, symbol_list: list[str]) -> set[str]:\n",
    "        '''\n",
    "            Calculate FIRST(X), where X is one or more non-terminals/terminals sequence\n",
    "\n",
    "            args\n",
    "                symbol_list: list of symbols\n",
    "            return\n",
    "                The FIRST set\n",
    "        '''\n",
    "\n",
    "        if (len(symbol_list) == 0):\n",
    "            return {\"epsilon\"}\n",
    "\n",
    "        res = set()\n",
    "        nullable = True\n",
    "        for symbol in symbol_list:\n",
    "            if is_nt(symbol):\n",
    "                for char in self.first[symbol]:\n",
    "                    if char != \"epsilon\":\n",
    "                        res.add(char)\n",
    "                \n",
    "                if symbol not in self.nullable:\n",
    "                    nullable = False\n",
    "                    break\n",
    "            else:\n",
    "                nullable = False\n",
    "                res.add(symbol)\n",
    "                break\n",
    "\n",
    "        if nullable:\n",
    "            res.add(\"epsilon\")\n",
    "        \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def init_FOLLOW(self, grammar: Grammar, start: str) -> dict[str, set[str]]:\n",
    "        '''\n",
    "            Calculate the FOLLOW set for all symbols\n",
    "            FOLLOW(A) is the set of terminals that can appear immediate after A\n",
    "            Example: S -> A a B c then a is in FOLLOW(A)\n",
    "\n",
    "            return\n",
    "                A dictionary with all the non-terminal as keys and FOLLOW sets as values\n",
    "        '''\n",
    "        follow: dict[str, set[str]] = {}\n",
    "        for nt in grammar:\n",
    "            follow[nt] = set()\n",
    "        follow[start].add(self.end_of_input)\n",
    "\n",
    "        changed = True\n",
    "        while (changed):\n",
    "            changed = False\n",
    "            for lhs in grammar:\n",
    "                for rhs in grammar[lhs]:\n",
    "                    for idx, symbol in enumerate(rhs):\n",
    "                        if not is_nt(symbol):\n",
    "                            continue\n",
    "                        \n",
    "                        # lhs -> ...By\n",
    "                        # Adding FIRST(y) to FOLLOW(B)\n",
    "                        # print(symbol + \" \" + str(rhs[idx + 1:]))\n",
    "                        first_y = self.calculate_FIRST(rhs[idx + 1:])\n",
    "                        for char in first_y:\n",
    "                            if char == \"epsilon\":\n",
    "                                continue\n",
    "                            if char not in follow[symbol]:\n",
    "                                changed = True\n",
    "                                follow[symbol].add(char)\n",
    "                        \n",
    "                        if \"epsilon\" in first_y:\n",
    "                            # Adding all symbol from FOLLOW(lhs) to FOLLOW(B)\n",
    "                            for char in follow[lhs]:\n",
    "                                if char not in follow[symbol]:\n",
    "                                    changed = True\n",
    "                                    follow[symbol].add(char)\n",
    "                        \n",
    "        return follow\n",
    "\n",
    "    def find_closure(self, items: list[Item]) -> set[Item]:\n",
    "        '''\n",
    "            Find the closure of a list of items\n",
    "\n",
    "            args\n",
    "                items: item list\n",
    "\n",
    "            return\n",
    "                The closure set of input items\n",
    "        '''\n",
    "        res = set(items)\n",
    "        prev_len = -1\n",
    "        while (prev_len != len(res)):\n",
    "            prev_len = len(res)\n",
    "            for item in sorted(list(res.copy())):\n",
    "                # dot at the end\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    continue\n",
    "\n",
    "                next_sym = item.rhs[item.dot]\n",
    "                if not is_nt(next_sym):\n",
    "                    continue\n",
    "                \n",
    "                # X -> α·Yβ, a\n",
    "                # Calculate FIRST(βa)\n",
    "                first_set = self.calculate_FIRST(item.rhs[item.dot + 1:] + [item.look_ahead])\n",
    "                \n",
    "                for production in self.grammar[next_sym]:\n",
    "                    for look_ahead in first_set:\n",
    "                        res.add(Item(next_sym, production, 0, look_ahead))\n",
    "        return res\n",
    "    \n",
    "    def transition(self, state: list[Item], next_sym: str) -> set[Item]:\n",
    "        '''\n",
    "            Calculate the transition from a state with <next_sym> edge\n",
    "\n",
    "            arg\n",
    "                state: current state\n",
    "                next_sym: transition edge, can be terminal or non-terminal\n",
    "            return\n",
    "                Next state, with all items in a set\n",
    "        '''        \n",
    "        items = []\n",
    "        for item in state:\n",
    "            if item.dot + 1 > len(item.rhs):\n",
    "                continue\n",
    "            if (item.rhs[item.dot] == next_sym):\n",
    "                new_item = copy(item)\n",
    "                new_item.dot = new_item.dot + 1\n",
    "                items.append(new_item)\n",
    "        \n",
    "        return self.find_closure(items)\n",
    "    \n",
    "    def generate_states(self) -> tuple[list[State], dict[tuple[int, str], int]]:\n",
    "        '''\n",
    "            This function generates all the states needed for the automata\n",
    "                - State format: a tuple (state_id, set of Items)\n",
    "\n",
    "            return\n",
    "                A tuple consists of\n",
    "                - A list of states\n",
    "                - A GOTO map: a dictionary, with (id, symbol) as keys and next_id as values\n",
    "                    Example: state_1 --A--> state_2, then GOTO[(1, \"A\")] = 2\n",
    "        '''\n",
    "        # Initial state has [<S#> -> ·<S>, $] and its closure\n",
    "        initial_state = (0, self.find_closure([Item(self.start, \n",
    "                                                    self.grammar[self.start][0], \n",
    "                                                    0, \n",
    "                                                    self.end_of_input)]))\n",
    "        states: list[State] = []\n",
    "        states.append(initial_state)\n",
    "        unprocessed_states = [initial_state]\n",
    "\n",
    "        goto_map: dict[tuple[int, str], int] = {}\n",
    "        while (len(unprocessed_states) > 0):\n",
    "            top_state_id, top_state = unprocessed_states.pop()\n",
    "            for item in sorted(list(top_state)):\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    continue\n",
    "\n",
    "                next_sym = item.rhs[item.dot]\n",
    "                next_state = self.transition(top_state, next_sym)\n",
    "\n",
    "                # Check if state already exist\n",
    "                duplicate = False\n",
    "                dup_idx = 0\n",
    "                for idx, state in states:\n",
    "                    if (all([(item in state) for item in next_state]) and \n",
    "                        len(next_state) == len(state)):\n",
    "                        dup_idx = idx\n",
    "                        duplicate = True\n",
    "                        break\n",
    "\n",
    "                if not duplicate:\n",
    "                    new_state = (len(states), next_state)\n",
    "                    states.append(new_state)\n",
    "                    unprocessed_states.append(new_state)\n",
    "                    goto_map[(top_state_id, next_sym)] = new_state[0]\n",
    "                else:\n",
    "                    goto_map[(top_state_id, next_sym)] = dup_idx\n",
    "\n",
    "        # Print result\n",
    "        if (LOGGING):\n",
    "            for state in states:\n",
    "                print(f\"State {state[0]}\")\n",
    "                for ins in state[1]:\n",
    "                    print(ins)\n",
    "                print(\"\")\n",
    "            \n",
    "            print(\"---------------\")\n",
    "            print(\"Transition map\")\n",
    "            for key, value in goto_map.items():\n",
    "                print(f\"GOTO {key} = {value}\")\n",
    "        \n",
    "        return (states, goto_map)\n",
    "    \n",
    "    def generate_parse_table(self):\n",
    "        '''\n",
    "            This function generates an LR(1) parse table for the given grammar\n",
    "\n",
    "            return\n",
    "                The parse table in the form of a 2-dimensional dictionary.\n",
    "                Usage: T[\\<state_id\\>][\\<symbol\\>], each item is a list of possible action either \"pk\" or \"r(A, p)\"\n",
    "        '''\n",
    "        states, goto_map = self.generate_states()\n",
    "\n",
    "        row_entries = [state[0] for state in states]\n",
    "        column_entries = (list(self.symbols) + [self.end_of_input])\n",
    "        column_entries.sort()\n",
    "        \n",
    "        table: dict[int, dict[str, list[str]]] = {}\n",
    "        # Init table\n",
    "        for row in row_entries:\n",
    "            table[row] = {}\n",
    "            for col in column_entries:\n",
    "                table[row][col] = []\n",
    "\n",
    "        # Add shift and goto\n",
    "        for state, symbol in goto_map.keys():\n",
    "            table[state][symbol].append(f\"p{goto_map[(state, symbol)]}\")\n",
    "\n",
    "        # Add reduce\n",
    "        for state_id, state in states:\n",
    "            for item in sorted(list(state)):\n",
    "                # Dot at the end\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    if item.lhs == self.start:\n",
    "                        table[state_id][self.end_of_input].append(\"acc\")\n",
    "                    else:\n",
    "                        action = f\"r{item.lhs}{item.dot}.{0}\"\n",
    "                        if (item.dot == 0):\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[item.lhs]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "                # Right-nulled\n",
    "                else:\n",
    "                    right_seq = item.rhs[item.dot:]\n",
    "                    if (all([sym in self.nullable for sym in right_seq])):\n",
    "                        label = ''.join(right_seq)\n",
    "                        action = \"\"\n",
    "                        if item.lhs == self.start:\n",
    "                            action = \"acc\"\n",
    "                        else:\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[label]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "\n",
    "        # print table\n",
    "        if (LOGGING):\n",
    "            print(\"\\nParsing table:\\n\")\n",
    "            frmt = \"{:>12}\" * len(column_entries)\n",
    "            print(\" \", frmt.format(*column_entries), \"\\n\")\n",
    "            ptr = 0\n",
    "            for state_id in row_entries:\n",
    "                # frmt1 = \"{:>8}\"\n",
    "                print(f\"{{:>3}}\".format('I'+str(state_id)), end=\"\")\n",
    "                for symbol in column_entries:\n",
    "                    list_opp = []\n",
    "                    for opp in table[state_id][symbol]:\n",
    "                        word = \"\"\n",
    "                        word += opp\n",
    "                        list_opp.append(word)\n",
    "                    print(f\"{{:>12}}\".format(\"/\".join(list_opp)), end=\"\")\n",
    "                print()\n",
    "        return table\n",
    "    \n",
    "    def export_to_csv(table, path: str):\n",
    "        # table = self\n",
    "        row_entries = list(table.keys())\n",
    "        symbols = list(table[0].keys())\n",
    "        header = [\"state\"] + list(table[0].keys())\n",
    "\n",
    "        with open(path, 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            for state_id in row_entries:\n",
    "                row = [str(state_id)]\n",
    "                for symbol in symbols:\n",
    "                    row.append('/'.join(table[state_id][symbol]))\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de47646-61e6-4a6e-b7ff-d92e3b269911",
   "metadata": {},
   "source": [
    "##### Helper functions to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585890d0-297f-48be-908e-1c243ba1727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoiceNode:\n",
    "    def __init__(self, parent, total):\n",
    "        self._p, self._chosen = parent, 0\n",
    "        self._total, self.next = total, None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(%s/%s %s)' % (str(self._chosen),\n",
    "                               str(self._total), str(self.next))\n",
    "\n",
    "    def __repr__(self): return repr((self._chosen, self._total))\n",
    "\n",
    "    def chosen(self): return self._chosen\n",
    "\n",
    "    def finished(self):\n",
    "        return self._chosen >= self._total\n",
    "    \n",
    "    def increment(self):\n",
    "        # as soon as we increment, next becomes invalid\n",
    "        self.next = None\n",
    "        self._chosen += 1\n",
    "        if self.finished():\n",
    "            if self._p is None: return None\n",
    "            return self._p.increment()\n",
    "        return self\n",
    "    \n",
    "\n",
    "class EnhancedExtractor:\n",
    "    def __init__(self, forest):\n",
    "        self.my_forest = forest\n",
    "        self.choices = ChoiceNode(None, 1)\n",
    "\n",
    "    def choose_path(self, arr_len, choices):\n",
    "        if choices.next is not None:\n",
    "            if choices.next.finished():\n",
    "                return None, choices.next\n",
    "        else:\n",
    "            choices.next = ChoiceNode(choices, arr_len)\n",
    "        next_choice = choices.next.chosen()\n",
    "        return next_choice, choices.next\n",
    "    \n",
    "    def extract_a_node(self, forest_node, seen, choices):\n",
    "        if isinstance(forest_node, SPPFNode):\n",
    "            if not forest_node.children:\n",
    "                return (forest_node.label, []), choices\n",
    "            \n",
    "            packing_node_children = isinstance(forest_node.children[0], PackingNode)\n",
    "\n",
    "            # PackingNode child\n",
    "            if packing_node_children:\n",
    "\n",
    "                child_ind, new_choices = self.choose_path(len(forest_node.children), choices)\n",
    "                \n",
    "                # out of choice\n",
    "                if child_ind is None:\n",
    "                    return None, new_choices \n",
    "                if str(id(forest_node.children[child_ind])) in seen:\n",
    "                    return None, new_choices\n",
    "                \n",
    "                n, newer_choices = self.extract_a_node(forest_node.children[child_ind], \n",
    "                                                       seen | {str(id(forest_node.children[child_ind]))}, \n",
    "                                                       new_choices)\n",
    "            \n",
    "                return (forest_node.label, n), newer_choices\n",
    "            \n",
    "            # SPPFNode child\n",
    "            list_n = []\n",
    "            for child in forest_node.children:\n",
    "                n, newer_choices = self.extract_a_node(\n",
    "                        child, seen | {str(id(child))}, choices)\n",
    "            \n",
    "                if n is None: return None, newer_choices\n",
    "                list_n.append(n)\n",
    "\n",
    "            return (forest_node.label, list_n), newer_choices\n",
    "\n",
    "\n",
    "        elif isinstance(forest_node, PackingNode):\n",
    "            cur_child_ind, new_choices = self.choose_path(len(forest_node.edges), choices)\n",
    "\n",
    "            # out of choice\n",
    "            if cur_child_ind is None:\n",
    "                return None, new_choices\n",
    "            if str(id(forest_node.edges[cur_child_ind])) in seen:\n",
    "                return None, new_choices \n",
    "\n",
    "            packing_node_children = isinstance(forest_node.edges[0], PackingNode)\n",
    "\n",
    "            # PackingNode child\n",
    "            if packing_node_children:\n",
    "\n",
    "                child_ind, new_choices = self.choose_path(len(forest_node.edges), choices)\n",
    "                \n",
    "                # out of choice\n",
    "                if child_ind is None:\n",
    "                    return None, new_choices\n",
    "                if str(id(forest_node.edges[child_ind])) in seen:\n",
    "                    return None, new_choices\n",
    "\n",
    "                \n",
    "                n, newer_choices = self.extract_a_node(forest_node.edges[child_ind], \n",
    "                                                       seen | {str(id(forest_node.edges[child_ind]))}, \n",
    "                                                       choices)\n",
    "            \n",
    "                return n, newer_choices\n",
    "            \n",
    "            # SPPFNode child\n",
    "            list_n = []\n",
    "            for child in forest_node.edges:\n",
    "                n, newer_choices = self.extract_a_node(\n",
    "                        child, seen | {str(id(child))}, choices)\n",
    "            \n",
    "                if n is None: return None, newer_choices\n",
    "                list_n.append(n)\n",
    "\n",
    "            return list_n, newer_choices\n",
    "        \n",
    "    def extract_a_tree(self):\n",
    "        choices = self.choices\n",
    "        while not self.choices.finished():\n",
    "            parse_tree, choices = self.extract_a_node(\n",
    "                    self.my_forest,\n",
    "                    set(), self.choices)\n",
    "            choices.increment()\n",
    "            if parse_tree is not None:\n",
    "                return parse_tree\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "class O:\n",
    "    def __init__(self, **keys): self.__dict__.update(keys)\n",
    "\n",
    "OPTIONS   = O(V='│', H='─', L='└', J = '├')\n",
    "\n",
    "def format_node(node):\n",
    "    key = node[0]\n",
    "    if key and (key[0], key[-1]) ==  ('<', '>'): return key\n",
    "    return repr(key)\n",
    "\n",
    "def get_children(node):\n",
    "    return node[1]\n",
    "\n",
    "def display_tree(node, format_node=format_node, get_children=get_children,\n",
    "                 options=OPTIONS):\n",
    "    print(format_node(node))\n",
    "    for line in format_tree(node, format_node, get_children, options):\n",
    "        print(line)\n",
    "\n",
    "def format_tree(node, format_node, get_children, options, prefix=''):\n",
    "    children = get_children(node)\n",
    "    if not children: return\n",
    "    *children, last_child = children\n",
    "    for child in children:\n",
    "        next_prefix = prefix + options.V + '   '\n",
    "        yield from format_child(child, next_prefix, format_node, get_children,\n",
    "                                options, prefix, False)\n",
    "    last_prefix = prefix + '    '\n",
    "    yield from format_child(last_child, last_prefix, format_node, get_children,\n",
    "                            options, prefix, True)\n",
    "\n",
    "def format_child(child, next_prefix, format_node, get_children, options,\n",
    "                 prefix, last):\n",
    "    sep = (options.L if last else options.J)\n",
    "    yield prefix + sep + options.H + ' ' + format_node(child)\n",
    "    yield from format_tree(child, format_node, get_children, options, next_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ba3fa",
   "metadata": {},
   "source": [
    "## Extending LR Parser\n",
    "### Eliminating Nondeterminism\n",
    "If you are familiar with LR, you probably know about *shift/reduce conflicts* (choices between shifting and reducing) and *reduce/reduce conflicts* (choices between reducing different rules), a normal LR parser cannot handle conflicts, as it does not know which choice to make. What we can do is incorporating a bit of breadth-first search, so the parser can try all options, and that is the main idea behind GLR.\n",
    "\n",
    "For example, consider the following ambiguous grammar:\n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ c & \\hspace{1cm} (1)\\\\\n",
    "S &\\rightarrow a\\ D \\ c &\\hspace{1cm} (2) \\\\\n",
    "B &\\rightarrow b &\\hspace{1cm} (3) \\\\\n",
    "D & \\rightarrow b &\\hspace{1cm} (4)\\end{split}$$\n",
    "For this grammar, the LR(1) automaton is as below:\n",
    "![sss](images/lr1_gram.png)\n",
    "And the LR(1) parse table is:\n",
    "\n",
    "| state | a   | b   | c               | $       | S   | B   | D   |\n",
    "| ----- | --- | --- | --------------- | ------- | --- | --- | --- |\n",
    "| 0     | p2  |     |                 |         | p1  |     |     |\n",
    "| 1     |     |     |                 | acc     |     |     |     |\n",
    "| 2     |     | p4  |                 |         |     | p5  | p3  |\n",
    "| 3     |     |     | p7              |         |     |     |     |\n",
    "| 4     |     |     | r(B, 3)/r(D, 4) |         |     |     |     |\n",
    "| 5     |     |     | p6              |         |     |     |     |\n",
    "| 6     |     |     |                 | r(S, 1) |     |     |     |\n",
    "| 7     |     |     |                 | r(S, 2) |     |     |     |\n",
    "\n",
    "In this table, \"$pk$\" is shift action, it means \"go to state $k$\" and $r(X, m)$ is the reduce action meaning \"reduce symbol $X$ with rule numbered $m$.\" The symbol $\\$$ is used to denote \"end of string.\" There is a reduce/reduce conflict in state 4. Let's see what happens when we try to parse the string \"$abc$\".\n",
    "\n",
    "| Step | Input | State | Stack                     | Next operation    |\n",
    "| ---- | ----- | ----- | ------------------------- | ----------------- |\n",
    "| 0    | \"\"    | 0     | $\\$, S_0$                 | $p2$              |\n",
    "| 1    | \"a\"   | 2     | $\\$, S_0, a, S_2$         | $p4$              |\n",
    "| 2    | \"ab\"  | 4     | $\\$, S_0, a, S_1, b, S_4$ | $r(B, 3)/r(D, 4)$ |\n",
    "\n",
    "A usual LR parser now has to choose between two possible reductions ($B \\rightarrow b$ and $D \\rightarrow b$). With GLR, it can attempt to try all options, but how would it do that? The simplest solution is to duplicate the stack and treat each stack as a separate process. After performing $r(B, 3)$ the stack is $\\{\\$, S_0, a, S_1, B, S_5\\}$; similarly, we obtain $\\{\\$, S_0, a, S_1, D, S_3\\}$ when $r(D, 4)$ is applied. Now we have 2 different stacks to manage, and the parse can continue to process with both stacks. However, this approach is not ideal, the number of stacks can blow up exponentially, we need something more efficient.\n",
    "### Graph-Structured Stack (GSS)\n",
    "In the above example, notice that the first four elements are the same in both stacks, therefore we can \"share\" them in a unified data structure. This is a \"Graph-Structured Stack\", or GSS, proposed by Tomita in his book [2]. \n",
    "![GSS_example](GSS_exam.png)\n",
    "\n",
    "This image illustrates how the states $S_0$ and $S_1$ are shared between the two stacks. As the name suggests, our stack is now a single graph, and each element in the stack is a node. In the original Tomita's approach, elements $a$, $D$ and $B$ are individual nodes, but here we have simplified by making them the edge labels between states. Each node contains a label, label is a state in the LR automaton (node $v_4$ has label $S_3$, or state 3 in the automaton).\n",
    "\n",
    "The nodes are divided into $n+1$ *levels,* with $n$ as the length of the input string. We call $U_i$ the set of nodes in level $i$, in the above example $U_0 = \\{v_1\\}$ and $U_1 = \\{v_2, v_3, v_4\\}$. GSS construction is done level by level, and a new level is created upon a *shift* action. The GSSNode data structure is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83729e7",
   "metadata": {
    "id": "P2ksOh3vYqN7"
   },
   "outputs": [],
   "source": [
    "class GSSNode:\n",
    "    '''\n",
    "        Represent a node in the GSS structure, nodes are identified by id\n",
    "    '''\n",
    "    def __init__(self, level: int, id: int, label):\n",
    "        self.level = level\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.children: list[tuple['GSSNode', 'SPPFNode']] = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"Node(v{self.id}, {self.label})\"\n",
    "        return repr\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    def add_child(self, child: 'GSSNode', edge):\n",
    "        self.children.append((child, edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cdb48",
   "metadata": {},
   "source": [
    "The GSS is a bit unusual. It does not perform the \"pop\" operation like an ordinary stack. Once a node is created, it is never removed. Instead of popping $m$ nodes out of the stack, we perform a traversal of length $m$ from the original node. For example, instead of popping 2 elements from node $v_4$, we traverse down the graph with length 2 and find that node $v_1$ is our target. We define a method to perform this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab1e491",
   "metadata": {
    "id": "TE1Ahk43f7Dc",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class GSSNode(GSSNode):\n",
    "    def find_paths_with_length(self, m: int) -> set[tuple['GSSNode',...]]:\n",
    "        '''\n",
    "            Find a set of nodes with length m from the origin node,\n",
    "            return tuples of lenght m in a set, tuples contain all the labels and the destination node\n",
    "        '''\n",
    "        \n",
    "        res: set[tuple] = set()\n",
    "        def dfs(node: GSSNode, path: list[GSSNode]):\n",
    "            if (len(path) >= m):\n",
    "                res.add(tuple(path + [node]))\n",
    "                return\n",
    "\n",
    "            for child, edge in node.children:\n",
    "                dfs(child, path + [edge])\n",
    "\n",
    "        dfs(self, [])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dd650",
   "metadata": {},
   "source": [
    "The `find_paths_with_length()` method doesn't have to account for cycle because GSS is a directed acyclic graph, a cycle cannot exist. Finally we can have our GSS class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e825deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSS:\n",
    "    '''\n",
    "        A Graph Structured Stack (GSS)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            Initialize the graph, in RNGLR, a GSS has n levels, where n is the length of input string\n",
    "\n",
    "            Each level is a set of GSSNodes, levels are stored in a list\n",
    "        '''\n",
    "        self.level: list[set[GSSNode]] = []\n",
    "        self.count = 0\n",
    "\n",
    "    def resize(self, n: int):\n",
    "        '''\n",
    "            Resize the GSS to include n levels\n",
    "        '''\n",
    "        self.level = [set() for i in range(n)]\n",
    "\n",
    "    def create_node(self, label, level: int):\n",
    "        '''\n",
    "            Create a new node with label in a specific level\n",
    "        '''\n",
    "        new_node = GSSNode(level, self.count, label)\n",
    "        self.count += 1\n",
    "        self.level[level].add(new_node)\n",
    "        return new_node\n",
    "    \n",
    "    def find_node(self, label, level: int) -> GSSNode:\n",
    "        '''\n",
    "            Find a node with label and in a specific level\n",
    "\n",
    "            return\n",
    "                GSSNode object if found, else None is returned\n",
    "        '''\n",
    "        # Can be optimized further\n",
    "        for node in self.level[level]:\n",
    "            if (node.label == label):\n",
    "                return node\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "            Print the GSS structure\n",
    "        '''\n",
    "        repr = \"GSS:\\n\"\n",
    "        for idx, level in enumerate(self.level):\n",
    "            repr += f\"Level {idx}:\\n\"\n",
    "            for node in level:\n",
    "                repr += f\"    {node}\\n\"\n",
    "                for child, edge in node.children:\n",
    "                    repr += f\"        {child} - {edge}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a889c75",
   "metadata": {},
   "source": [
    "### Shared Packed Parse Forest (SPPF)\n",
    "For practical usage, we are more interested in a full parser rather than just a recogniser. While a recogniser's output is simply a yes/no answer, a parser has to provide a full derivation path (usually in the form of the parse tree). However, a parse tree is insufficient because we are dealing with all context-free grammars, which includes ambiguous grammars; thus, multiple derivations (or even infinite ones) are possible. Instead of a parse tree, a data structure called *Shared Packed Parse Forest* (SPPF) is used.\n",
    "\n",
    "Consider the string \"abc\" in the above example, we have 2 possible derivations, resulting in 2 parse trees: \n",
    "![parse tree](images/Parse_tree.png)\n",
    "In an SPPF, we combine them into a single graph, the final result looks like this\n",
    "![sppf](images/SPPF.png)\n",
    "\n",
    "Nodes like \"S\", \"a\", \"b\" and \"c\" are shared to reduce space. Since $S$ can be derived in two ways (either $S\\rightarrow a\\ B\\ c$ or $S\\rightarrow a\\ D\\ c$), two new black nodes are created to represent different choices. These are called **packing nodes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a53d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackingNode:\n",
    "    def __init__(self):\n",
    "        self.edges = []\n",
    "\n",
    "    def add_edge(self, node):\n",
    "        self.edges.append(node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PackingNode({self.edges})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404079c1",
   "metadata": {},
   "source": [
    "SPPF nodes are identified by `(label, start_position)`. Each node represents one non-terminal in the derivation process, and its children are the product of derivation step. The `start_position` parameter is used to differentiate between the same non-terminals that may occur multiple times during the parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bdd6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPFNode:\n",
    "    def __init__(self, id: int, label: str, start_pos:int = -1):\n",
    "        '''\n",
    "            start_pos = -1 means the node is in epsilon-SPPF\n",
    "        '''\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.start_pos = start_pos\n",
    "        self.children: list['SPPFNode' | PackingNode] = []\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "    \n",
    "    def check_sequence_exists(self, nodes: list['SPPFNode']) -> bool:\n",
    "        '''\n",
    "            Check if a sequence of nodes already exists in the current node\n",
    "        '''\n",
    "        \n",
    "        # If packing nodes exist\n",
    "        if any(isinstance(child, PackingNode) for child in self.children):\n",
    "            for child in self.children:\n",
    "                if child.edges == nodes:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        # No packing nodes case\n",
    "        return self.children == nodes\n",
    "    \n",
    "    # Nodes are identified by (label, start_pos)\n",
    "    def __hash__(self):\n",
    "        return hash((self.label, self.start_pos))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.label == other.label and self.start_pos == other.start_pos)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.label == \"\":\n",
    "            return f\"SPPF Node:(blank)\"\n",
    "        return f\"SPPF Node:({self.label}, {self.start_pos})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ccccfa",
   "metadata": {},
   "source": [
    "For our SPPFNode class, we also want an `add_children()` method, its purpose is to maintain the following property for every node in the SPPF:\n",
    "- Each choice is unique, there is no overlapping choice.\n",
    "- If there is only one possible choice, no *packing node* is used.\n",
    "- If there are at least 2 choices, all choices must be wrapped in *packing nodes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4ad51f-54fc-421d-ab39-87fb66e44bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPFNode(SPPFNode):\n",
    "    def add_children(self, nodes: list['SPPFNode']):\n",
    "        '''\n",
    "            Add a list of nodes to the current node\n",
    "        '''\n",
    "        if len(self.children) == 0:\n",
    "            for node in nodes:\n",
    "                self.add_child(node)\n",
    "            return\n",
    "        \n",
    "        # If already exists, we skip\n",
    "        if self.check_sequence_exists(nodes):\n",
    "            return\n",
    "        \n",
    "        # No packing node yet\n",
    "        if not isinstance(self.children[0], PackingNode):\n",
    "            z = PackingNode()\n",
    "            for child in self.children:\n",
    "                z.add_edge(child)\n",
    "            self.children = [z]\n",
    "        \n",
    "        t = PackingNode()\n",
    "        for node in nodes:\n",
    "            t.add_edge(node)\n",
    "        self.children.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb5024-ec4e-4dfb-8727-17f28defe9af",
   "metadata": {},
   "source": [
    "And finally, we can define the SPPF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d615457e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class SPPF:\n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "        # Two dictionary node_id -> Node and node_label -> node_id\n",
    "        self.epsilon_sppf, self.I = self.build_epsilon_sppf()\n",
    "\n",
    "        self.nodes: list[SPPFNode] = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def create_node(self, label: str, start_pos: int) -> SPPFNode:\n",
    "        node = SPPFNode(self.counter, label, start_pos)\n",
    "        self.counter += 1\n",
    "        self.nodes.append(node)\n",
    "        return node\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = \"SPPF:\\n\"\n",
    "        for node in self.nodes:\n",
    "            if node.label == \"\":\n",
    "                repr += f\"    blank\\n\"\n",
    "            else:\n",
    "                repr += f\"    {node.label}-{node.start_pos}\\n\"\n",
    "            for child in node.children:\n",
    "                if isinstance(child, PackingNode):\n",
    "                    repr += f\"        PackingNode\\n\"\n",
    "                    for edge in child.edges:\n",
    "                        repr += f\"            {edge}\\n\"\n",
    "                else: repr += f\"        {child}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b6104",
   "metadata": {},
   "source": [
    "#### Epsilon SPPF\n",
    "An SPPF tree for a nullable string or symbol is called *epsilon-SPPF* (or $\\epsilon$-SPPF). We precompute $\\epsilon$-SPPF trees for nullable non-terminals ($A \\overset{*}\\rightarrow \\epsilon$), this step is necessary for our parser later. In addition to non-terminals, we also build an $\\epsilon$-SPPF tree for every string $\\beta$ such that $\\beta\\overset{*}\\rightarrow \\epsilon$ and there exists a rule $A \\rightarrow \\alpha \\beta$ ($\\alpha \\neq \\epsilon$) in the grammar, such string $\\beta$ is also called *right-nullable*. Finally, we define $I$ as an index function which accepts a non-terminal/string and return the corresponding $\\epsilon$-SPPF root.\n",
    "\n",
    "Let's look at an example, grammar 5.3 in the dissertation by Economopoulos:\n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ B \\ C\\\\\n",
    "B &\\rightarrow b \\ |\\ \\epsilon \\\\\n",
    "C & \\rightarrow \\epsilon\\end{split}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc28f1d2-9df7-427b-a42c-99bfec42c25c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grammar_53 = {\n",
    "    \"<S>\": [[\"a\", \"<B>\", \"<B>\", \"<C>\"]],\n",
    "    \"<B>\": [[\"b\"], []],\n",
    "    \"<C>\": [[]]\n",
    "}\n",
    "start_symbol=\"<S>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b015f-4508-44e2-9ca0-54ed76cb68fd",
   "metadata": {},
   "source": [
    "We have $B$ and $C$ as nullable non-terminals, and the strings $BBC$ and $BC$ satisfy the conditions for $\\beta$. Therefore we build the $\\epsilon$-SPPF for $B$, $C$, $BB$ and $BBC$:\n",
    "![sppf_epsilon](images/SPPFepsilon.png)\n",
    "In this tree, vertices are indexed from 1 to 4, hence, our $I$ function can be defined with $I(B) = 1$, $I(C) = 2$, $I(BBC)=3$ and $I(BC)=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33640c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPF(SPPF):\n",
    "\tdef build_epsilon_sppf(self) -> tuple[dict[int, SPPFNode], dict[str, int]]:\n",
    "\t        '''\n",
    "\t            Build an epsilon-SPPF tree\n",
    "\t\n",
    "\t            return\n",
    "\t                A tuple that contains\n",
    "\t                - All SPPFNodes created, stored in a dict\n",
    "\t                - The I function dictionary\n",
    "\t        '''\n",
    "\t        # key: node_id, value: SPPF Node\n",
    "\t        epsilon_sppf: dict[int, SPPFNode] = {}\n",
    "\t\n",
    "\t        # Create epsilon node\n",
    "\t        eps_node = SPPFNode(0, \"epsilon\")\n",
    "\t        epsilon_sppf[0] = eps_node\n",
    "\t        counter = 1\n",
    "\t\n",
    "\t        # Find a given node with label\n",
    "\t        node_with_label: dict[str, SPPFNode] = {}\n",
    "\t\n",
    "\t        nullable = TableGenerator.get_nullable(self.grammar)\n",
    "\t\n",
    "\t        # Step 1, add all nullable symbols\n",
    "\t        # Sorted to guarantee determinism\n",
    "\t        for nt in sorted(nullable):\n",
    "\t            node = SPPFNode(counter, nt)\n",
    "\t            epsilon_sppf[counter] = node\n",
    "\t            node_with_label[nt] = node\n",
    "\t            counter += 1\n",
    "\t        \n",
    "\t        for lhs in self.grammar:\n",
    "\t            for rhs in self.grammar[lhs]:\n",
    "\t                # Epsilon rule\n",
    "\t                if len(rhs) == 0:\n",
    "\t                    node_with_label[lhs].add_child(eps_node)\n",
    "\t                # Total nullable\n",
    "\t                elif all(x in nullable for x in rhs):\n",
    "\t                    node = PackingNode()\n",
    "\t                    for nt in rhs:\n",
    "\t                        node.add_edge(node_with_label[nt])\n",
    "\t                    node_with_label[lhs].add_child(node)\n",
    "\t                # Partial nullable\n",
    "\t                else:\n",
    "\t                    for i in range(1, len(rhs)):\n",
    "\t                        partial_rhs = rhs[i:]\n",
    "\t                        if len(partial_rhs) == 0:\n",
    "\t                            continue\n",
    "\t\n",
    "\t                        if all(x in nullable for x in partial_rhs):\n",
    "\t                            label = ''.join(partial_rhs)\n",
    "\t                            if label in node_with_label:\n",
    "\t                                continue\n",
    "\t                            node = SPPFNode(counter, label)\n",
    "\t                            for x in partial_rhs:\n",
    "\t                                node.add_child(node_with_label[x])\n",
    "\t                            node_with_label[label] = node\n",
    "\t                            epsilon_sppf[counter] = node\n",
    "\t                            counter += 1\n",
    "\t\n",
    "\t        # Construct the I indexing map label -> node_id\n",
    "\t        I: dict[str, int] = {}\n",
    "\t        for label, node in node_with_label.items():\n",
    "\t            I[label] = node.id\n",
    "\t        \n",
    "\t        return (epsilon_sppf, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1d062-4ec0-4c6f-abe6-38810185e192",
   "metadata": {},
   "source": [
    "Using the code to build the $\\epsilon$-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0670dd00-f488-473e-a5c7-e894d2f251a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: epsilon\n",
      "1: <B>\n",
      "    epsilon\n",
      "2: <C>\n",
      "    epsilon\n",
      "3: <B><B><C>\n",
      "    <B>\n",
      "    <B>\n",
      "    <C>\n",
      "4: <B><C>\n",
      "    <B>\n",
      "    <C>\n"
     ]
    }
   ],
   "source": [
    "sppf = SPPF(grammar_53)\n",
    "for node_id, node in sppf.epsilon_sppf.items():\n",
    "    print(f\"{node_id}: {node.label}\")\n",
    "    for child in node.children:\n",
    "        print(f\"    {child.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99342dad",
   "metadata": {},
   "source": [
    "## Right-Nulled GLR (RNGLR)\n",
    "In Tomita's book, he introduced 4 different algorithms. The first one only works for grammar without $\\epsilon$-rules. Algorithm 2 and 3 were intended to handle $\\epsilon$-rules but failed to deal with hidden left-recursion in grammars. Algorithm 4 (which is the full parser) inherited the same problem from algorithm 2 and 3. RNGLR is an extension to algorithm 1 to include grammars with $\\epsilon$-rules. \n",
    "#### Right-nulled parse table\n",
    "Our algorithm uses a slightly modified parse table, which is neither LR(1) nor LALR(1). This table is built upon the usual LR table, but with the addition of new reductions for \"*right-nullable*\" rules; therefore it is called *right-nulled parse table*. A *right-nullable* rule has the form $A\\rightarrow \\alpha\\beta$, where $\\beta$ can derive to $\\epsilon$. If an reduction *item* is of the form ($A\\rightarrow \\alpha \\cdot\\beta, a$), we write $r(A, m, f)$ into the parse table, with $m=|\\alpha|$ and $f=I(\\beta)$ if $m\\neq0$ and $f=I(A)$ if $m=0$.\n",
    "\n",
    "Back to grammar 5.3 \n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ B \\ C\\\\\n",
    "B &\\rightarrow b \\ |\\ \\epsilon \\\\\n",
    "C & \\rightarrow \\epsilon\\end{split}$$\n",
    "![automaton](images/grammar_53_auto.png)\n",
    "\n",
    "The regular LR(1) parse table for this grammar is \n",
    "\n",
    "| State | B   | C   | S   | a   | b             | $          |\n",
    "| ----- | --- | --- | --- | --- | ------------- | ---------- |\n",
    "| 0     |     |     | p1  | p2  |               |            |\n",
    "| 1     |     |     |     |     |               | acc        |\n",
    "| 2     | p4  |     |     |     | p3/r(B, 0, 1) | r(B, 0, 1) |\n",
    "| 3     |     |     |     |     | r(B, 1, 0)    | r(B, 1, 0) |\n",
    "| 4     | p6  |     |     |     | p5            | r(B, 0, 1) |\n",
    "| 5     |     |     |     |     |               | r(B, 1, 0) |\n",
    "| 6     |     | p7  |     |     |               | r(C, 0, 2) |\n",
    "| 7     |     |     |     |     |               | r(S, 4, 0) |\n",
    "\n",
    "At cell $T(6, \\$)$, we can see that the parser is performing the reduction $C \\rightarrow \\epsilon$, in this case $m = |\\alpha| = 0$ and $I(C) = 2$, hence we write $r(C, 0, 2)$. To form a *right-nulled* parse table, we need to add more reductions for right-nullable items. As the strings $BBC$ and $BC$ are nullable, such items in this case are $S\\rightarrow a\\cdot B\\ B\\ C$, $S\\rightarrow a\\ B\\cdot B\\ C$ and $S\\rightarrow a\\ B\\ B \\cdot C$. Three new reductions are added into the *right-nulled table*:\n",
    "\n",
    "| State | B   | C   | S   | a   | b             | $                     |\n",
    "| ----- | --- | --- | --- | --- | ------------- | --------------------- |\n",
    "| 0     |     |     | p1  | p2  |               |                       |\n",
    "| 1     |     |     |     |     |               | acc                   |\n",
    "| 2     | p4  |     |     |     | p3/r(B, 0, 1) | r(B, 0, 1)/r(S, 1, 3) |\n",
    "| 3     |     |     |     |     | r(B, 1, 0)    | r(B, 1, 0)            |\n",
    "| 4     | p6  |     |     |     | p5            | r(B, 0, 1)/r(S,2,4)   |\n",
    "| 5     |     |     |     |     |               | r(B, 1, 0)            |\n",
    "| 6     |     | p7  |     |     |               | r(C, 0, 2)/r(S, 3, 2) |\n",
    "| 7     |     |     |     |     |               | r(S, 4, 0)            |\n",
    "\n",
    "We have the generate parse table method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5aa403c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class TableGenerator(TableGenerator):\n",
    "    def generate_parse_table(self):\n",
    "        '''\n",
    "            This function generates an LR(1) parse table for the given grammar\n",
    "\n",
    "            return\n",
    "                The parse table in the form of a 2-dimensional dictionary.\n",
    "                Usage: T[\\<state_id\\>][\\<symbol\\>], each item is a list of possible action either \"pk\" or \"r(A, p)\"\n",
    "        '''\n",
    "        states, goto_map = self.generate_states()\n",
    "\n",
    "        row_entries = [state[0] for state in states]\n",
    "        column_entries = (list(self.symbols) + [self.end_of_input])\n",
    "        column_entries.sort()\n",
    "        \n",
    "        table: dict[int, dict[str, list[str]]] = {}\n",
    "        # Init table\n",
    "        for row in row_entries:\n",
    "            table[row] = {}\n",
    "            for col in column_entries:\n",
    "                table[row][col] = []\n",
    "\n",
    "        # Add shift and goto\n",
    "        for state, symbol in goto_map.keys():\n",
    "            table[state][symbol].append(f\"p{goto_map[(state, symbol)]}\")\n",
    "\n",
    "        # Add reduce\n",
    "        for state_id, state in states:\n",
    "            for item in state:\n",
    "                # Dot at the end\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    if item.lhs == self.start:\n",
    "                        table[state_id][self.end_of_input].append(\"acc\")\n",
    "                    else:\n",
    "                        action = f\"r{item.lhs}{item.dot}.{0}\"\n",
    "                        if (item.dot == 0):\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[item.lhs]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "                # Right-nulled\n",
    "                else:\n",
    "                    right_seq = item.rhs[item.dot:]\n",
    "                    if (all([sym in self.nullable for sym in right_seq])):\n",
    "                        label = ''.join(right_seq)\n",
    "                        action = \"\"\n",
    "                        if item.lhs == self.start:\n",
    "                            action = \"acc\"\n",
    "                        else:\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[label]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "\n",
    "        # print table\n",
    "        if (LOGGING):\n",
    "            print(\"\\nParsing table:\\n\")\n",
    "            frmt = \"{:<12}\" * len(column_entries)\n",
    "            print(\"    \", frmt.format(*column_entries), \"\\n\")\n",
    "            ptr = 0\n",
    "            for state_id in row_entries:\n",
    "                print(f\"{{:<5}}\".format('I'+str(state_id)), end=\"\")\n",
    "                for symbol in column_entries:\n",
    "                    list_opp = []\n",
    "                    for opp in table[state_id][symbol]:\n",
    "                        word = \"\"\n",
    "                        word += opp\n",
    "                        list_opp.append(word)\n",
    "                    print(f\"{{:<12}}\".format(\"/\".join(list_opp)), end=\"\")\n",
    "                print()\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11612cc-e4f4-4f58-9cf8-475e936637dc",
   "metadata": {},
   "source": [
    "Let us use it to generate a rigth-nulled parse table for grammar 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae057c24-74aa-4795-a8a2-625e0918e1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0\n",
      "<S'> ->  · <S>, €\n",
      "<S> ->  · a <B> <B> <C>, €\n",
      "\n",
      "State 1\n",
      "<S'> -> <S> · , €\n",
      "\n",
      "State 2\n",
      "<B> ->  · b, b\n",
      "<B> ->  · , b\n",
      "<B> ->  · b, €\n",
      "<S> -> a · <B> <B> <C>, €\n",
      "<B> ->  · , €\n",
      "\n",
      "State 3\n",
      "<B> -> b · , b\n",
      "<B> -> b · , €\n",
      "\n",
      "State 4\n",
      "<S> -> a <B> · <B> <C>, €\n",
      "<B> ->  · b, €\n",
      "<B> ->  · , €\n",
      "\n",
      "State 5\n",
      "<B> -> b · , €\n",
      "\n",
      "State 6\n",
      "<C> ->  · , €\n",
      "<S> -> a <B> <B> · <C>, €\n",
      "\n",
      "State 7\n",
      "<S> -> a <B> <B> <C> · , €\n",
      "\n",
      "---------------\n",
      "Transition map\n",
      "GOTO (0, '<S>') = 1\n",
      "GOTO (0, 'a') = 2\n",
      "GOTO (2, 'b') = 3\n",
      "GOTO (2, '<B>') = 4\n",
      "GOTO (4, 'b') = 5\n",
      "GOTO (4, '<B>') = 6\n",
      "GOTO (6, '<C>') = 7\n",
      "\n",
      "Parsing table:\n",
      "\n",
      "     <B>         <C>         <S>         a           b           €            \n",
      "\n",
      "I0                           p1          p2                                  \n",
      "I1                                                               acc         \n",
      "I2   p4                                              p3/r<B>0.1  r<S>1.3/r<B>0.1\n",
      "I3                                                   r<B>1.0     r<B>1.0     \n",
      "I4   p6                                              p5          r<S>2.4/r<B>0.1\n",
      "I5                                                               r<B>1.0     \n",
      "I6               p7                                              r<C>0.2/r<S>3.2\n",
      "I7                                                               r<S>4.0     \n"
     ]
    }
   ],
   "source": [
    "LOGGING = True\n",
    "\n",
    "generator = TableGenerator(grammar_53, start_symbol)\n",
    "table = generator.generate_parse_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64587276-49c6-464e-9ba6-e212cb1727db",
   "metadata": {},
   "source": [
    "In the code, we are storing the reductions as `rBm.k`. The non-terminal `B` is enclosed by `<>` following the fuzzingbook format, $m$ and $k$ are separated by a dot. For this we define a function to parse the action string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ad5f08-58d7-47bc-afdf-40d6c7756510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(action: str) -> tuple[str, ...]:\n",
    "        '''\n",
    "            Parse the action string\n",
    "            \n",
    "            args\n",
    "                action: the action string, either \"pk\" or \"r\\<A\\>m.f\"\n",
    "            \n",
    "            return\n",
    "                - pk -> (\"p\", k)\n",
    "                - r<A>m.f -> (\"r\", \"\\<A\\>\", m, f)\n",
    "                - m, f are integers\n",
    "        '''\n",
    "        if (action == \"acc\"):\n",
    "            return (\"acc\",)\n",
    "        action_char = action[0]\n",
    "        assert(action_char == 'p' or action_char == 'r')\n",
    "\n",
    "        if (action_char == 'p'):\n",
    "            number = int(action[1:])\n",
    "            return (action_char, number)\n",
    "\n",
    "        # Case \"r\"\n",
    "        first_idx = action.find(\"<\")\n",
    "        last_idx = action.rfind(\">\")\n",
    "        symbol = action[first_idx:last_idx + 1]\n",
    "        dot_separator = action.rfind(\".\")\n",
    "        number_1 = int(action[last_idx + 1:dot_separator])\n",
    "        number_2 = int(action[dot_separator + 1:])\n",
    "        return (action_char, symbol, number_1, number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d86769",
   "metadata": {},
   "source": [
    "#### The RNGLR parser\n",
    "With GSS, SPPF and the parse table ready, we are now ready to build the RNGLR parser. The parser processes input string one by one, for each symbol a new GSS level is created, it then performs every possible reduction before shifting. Reduction and shift are performed by $\\mathrm{Reducer}$ and $\\mathrm{Shifter}$ respectively. Two special bookkeeping sets $\\mathcal{Q}$ and $\\mathcal{R}$ are used to store pending shift and reduction actions. In general:\n",
    "- $\\mathcal{Q}$ stores the shift actions in the form of $(v, k)$, which means \"from node $v$ to go state $k$\" where $k$ is a state in the LR automaton and $v$ is a node in GSS. Elements in $\\mathcal{Q}$ are processed by the $\\textrm{Shifter}$.\n",
    "- $\\mathcal{R}$ stores the reduction actions. Whenever a new edge between $v$ and $w$ is created in the GSS, all applicable reductions from $v$ are processed. For a reduction $r(X, m, f)$, we add $(w, X, m, f, z)$ into $\\mathcal{R}$ where $z$ is the SPPF node that between $v$ and $w$, if $m = 0$ then $z$ is the $\\epsilon$ node.\n",
    "\n",
    "In addition to $\\mathcal{Q}$ and $\\mathcal{R}$, a set $\\mathcal{N}$ is also used to bookkeep SPPF nodes, set $\\mathcal{N}$ is reset after each parser iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e9ad7e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class RNGLRParser:\n",
    "    '''\n",
    "        The RNGLR parser\n",
    "    '''\n",
    "    def __init__(self, start: str, grammar: Grammar, table: dict[int, dict[str, list[str]]]):\n",
    "        '''\n",
    "            Initialize RNGLR\n",
    "        '''\n",
    "        self.start = self.augment_start(start)\n",
    "        self.grammar = grammar\n",
    "        self.table = table\n",
    "\n",
    "        self.input_str = \"\"\n",
    "        self.end_of_input = \"€\"\n",
    "        self.gss = GSS()\n",
    "\n",
    "        # R and Q set, respectively\n",
    "        self.reductions: list[tuple[GSSNode, str, int]] = []\n",
    "        self.shifts: list[tuple[GSSNode, int]] = []\n",
    "\n",
    "        self.accept_states: set[int] = self.get_accept_states()\n",
    "        \n",
    "        self.sppf = SPPF(grammar)\n",
    "        self.set_N: dict[tuple[str, int], SPPFNode] = {}\n",
    "    \n",
    "    def augment_start(self, start: str):\n",
    "        '''\n",
    "            Reformat the start symbol to <S#>\n",
    "        '''\n",
    "        new_start = ''.join([start[:-1], \"'\", start[-1:]])\n",
    "        return new_start\n",
    "\n",
    "    def get_accept_states(self) -> set[int]:\n",
    "        '''\n",
    "            Get the accept states from the parsing table\n",
    "        '''\n",
    "        ret = set()\n",
    "        for state_id in self.table:\n",
    "            if self.end_of_input in self.table[state_id]:\n",
    "                for action in self.table[state_id][self.end_of_input]:\n",
    "                    if action == \"acc\":\n",
    "                        ret.add(state_id)\n",
    "        return ret\n",
    "\n",
    "    def add_reduction(self, v: GSSNode, X: str, m: int, f:int, z: 'SPPFNode'):\n",
    "        # print(\"     Added reduction: \", v, X, m, f, z)\n",
    "        self.reductions.append((v, X, m, f, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1bade",
   "metadata": {},
   "source": [
    "##### Pseudocode and implementation\n",
    "$U_i$ is level $i$ in the GSS \n",
    "**Input:** string $a_0a_1\\dots a_{n-1}$, start state $S_S$, accept state $S_A$, table $T$ \n",
    "**Parse($S$)**\n",
    "- If $n$ is 0\n",
    "\t- If $acc \\in T(S_S, \\$)$\n",
    "\t\t- return success\n",
    "\t- return failure\n",
    "- Else\n",
    "\t- Initialisation\n",
    "\t- Look at $T(S_S, a_0)$\n",
    "\t\t- Add all applicable shift actions to $\\mathcal{Q}$\n",
    "\t\t- Add all applicable reduce actions to $\\mathcal{R}$\n",
    "\t- For $i = 0$ to $n$ do\n",
    "\t\t- If $U_i$ is not empty\n",
    "\t\t\t- $\\mathcal{N} \\gets \\emptyset$\n",
    "\t\t\t- While $\\mathcal{R} \\ne \\emptyset$\n",
    "\t\t\t\t- $\\textrm{Reducer}(i)$\n",
    "\t\t\t- $\\textrm{Shifter(i)}$\n",
    "\t- If $S_A \\in U_n$\n",
    "\t\t- set SPPF root\n",
    "\t\t- return success\n",
    "\t- return failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNGLRParser(RNGLRParser):\n",
    "    def parse(self, input_str: str):\n",
    "        '''\n",
    "            The RNGLR recongizer, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "        '''\n",
    "        sppf_root = None\n",
    "        result = False\n",
    "\n",
    "        if (len(input_str) == 0):\n",
    "            if \"acc\" in self.table[0][self.end_of_input]:\n",
    "                sppf_root = self.sppf.epsilon_sppf[self.sppf.I[self.start]]\n",
    "                result = True\n",
    "        else:\n",
    "            # Init step\n",
    "            input_str = input_str + self.end_of_input\n",
    "            self.input_str = input_str\n",
    "            self.gss.resize(len(input_str))\n",
    "            v_0 = self.gss.create_node(0, 0)\n",
    "\n",
    "            # Check T(S, a_0)\n",
    "            for action in self.table[0][input_str[0]]:\n",
    "                action = get_action(action)\n",
    "                if (action[0] == 'p'):\n",
    "                    self.shifts.append((v_0, action[1]))\n",
    "                if (action[0] == 'r'):\n",
    "                    # Reduce\n",
    "                    if (action[2] == 0):\n",
    "                        # Add (v_0, X, 0, f, epsilon)\n",
    "                        self.add_reduction(v_0, action[1], 0, action[2], self.sppf.epsilon_sppf[0])\n",
    "\n",
    "            # Now we parse\n",
    "            for i in range(len(input_str)):\n",
    "                if len(self.gss.level[i]) > 0:\n",
    "                    self.set_N = {}\n",
    "                    while len(self.reductions) > 0:\n",
    "                        self.reducer(i)\n",
    "                    self.shifter(i)\n",
    "\n",
    "                    # if len(self.reductions) == 0:\n",
    "                    #     break\n",
    "            # Check accept state\n",
    "            for state in self.accept_states:\n",
    "                acc_node = self.gss.find_node(state, len(input_str) - 1)\n",
    "                if acc_node is not None:\n",
    "                    result = True\n",
    "\n",
    "                    # Find SPPF root\n",
    "                    for child in acc_node.children:\n",
    "                        if child[0].label == v_0.label:\n",
    "                            sppf_root = child[1]\n",
    "                    \n",
    "                    # print(f\"SPPF root: {sppf_root}\")\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        # print(self.gss)\n",
    "        return (result, sppf_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5dcb16",
   "metadata": {},
   "source": [
    "**Reducer($i$)**\n",
    "- Pop top element $(v, X, m, f, y)$ from $\\mathcal{R}$\n",
    "- Find all the paths in the GSS with length $\\max(m - 1, 0)$ starting from $v$, call the set of paths $\\mathcal{X}$\n",
    "- For $path$ in $\\mathcal{X}$\n",
    "\t- Let $u$ be the final node in $path$\n",
    "\t- if $m = 0$\n",
    "\t\t- $z\\gets$ node $f$ in the $\\epsilon$-SPPF tree\n",
    "\t- else\n",
    "\t\t- Let $c$ be the level of $u$ in GSS\n",
    "\t\t- Find SPPF node $z = (X, c)$ in $\\mathcal{N}$\n",
    "\t\t\t- If not exist then create $z$ and add to $\\mathcal{N}$\n",
    "\t- Let $k$ be the label of $u$ and $pl$ be the shift action in $T(k, a_i)$\n",
    "\t- If exists node $w$ with label $l$ in $U_i$\n",
    "\t\t- If edge $(w, u)$ does not exist\n",
    "\t\t\t- Create edge $(w, u)$ with label $z$\n",
    "\t\t\t- For $r(B, t, f) \\in T(l, a_i)$\n",
    "\t\t\t\t- If $t \\neq 0$ add $(u, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t- Else\n",
    "\t\t- Create node $w$\n",
    "\t\t- Create edge $(w, u)$ with label $z$\n",
    "\t\t- For action in $T(l, a_i)$\n",
    "\t\t\t- If shift action $ph$\n",
    "\t\t\t\t- Add $(w, h)$ to $\\mathcal{Q}$\n",
    "\t\t\t- If reduce action $r(B, t, f)$\n",
    "\t\t\t\t- If $t = 0$\n",
    "\t\t\t\t\t- Add $(w, B, t, f, \\epsilon)$ to $\\mathcal{R}$\n",
    "\t\t\t\t- if $t\\ne 0$ and $m\\neq 0$\n",
    "\t\t\t\t\t- Add $(w, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t- If $m\\neq 0$\n",
    "\t\t- $nodeSequence \\gets$ $w_{m-1},\\dots,w_1$ be the edge labels on the path\n",
    "\t\t- Append $y$ to $nodeSequence$\n",
    "\t\t- If $f\\ne 0$\n",
    "\t\t\t- Append $\\epsilon$-SPPF node numbered $f$ to $nodeSequence$\n",
    "\t\t- Call $z.\\textrm{addChildren}(nodeSequence)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e0b7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNGLRParser(RNGLRParser):\n",
    "    def reducer(self, i: int):\n",
    "        '''\n",
    "    \t\tThe reducer, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "    \t'''\n",
    "        v, X, m, f, y = self.reductions.pop()\n",
    "        \n",
    "    \t# find the set of nodes that can be reached from v with lenght m-1\n",
    "        paths = v.find_paths_with_length(max(0, m - 1))\n",
    "        z: SPPFNode = None\n",
    "    \n",
    "        for path in paths:\n",
    "            u = path[-1]\n",
    "            k = u.label\n",
    "    \n",
    "            if m == 0:\n",
    "                z = self.sppf.epsilon_sppf[f]\n",
    "            else:\n",
    "                c = u.level\n",
    "                if (X, c) not in self.set_N:\n",
    "                    z = self.sppf.create_node(X, c)\n",
    "                    self.set_N[X, c] = z\n",
    "                else:\n",
    "                    z = self.set_N[(X, c)]\n",
    "            for action in self.table[k][X]:\n",
    "                action_obj = get_action(action)\n",
    "                if action_obj[0] == 'p':\n",
    "                    w = self.gss.find_node(action_obj[1], i)\n",
    "                    if w is not None:\n",
    "                        if u not in [x[0] for x in w.children]:\n",
    "                            w.add_child(u, z)\n",
    "                            if m != 0:\n",
    "                                for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                                    action_obj = get_action(action)\n",
    "                                    if action_obj[0] == 'r' and action_obj[2] != 0:\n",
    "                                        # (u, B, t, f, z)\n",
    "                                        self.add_reduction(u, action_obj[1], action_obj[2], action_obj[3], z)\n",
    "                    else:\n",
    "                        w = self.gss.create_node(action_obj[1], i)\n",
    "                        w.add_child(u, z)\n",
    "                        for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                            action_obj = get_action(action)\n",
    "                            if action_obj[0] == 'p':\n",
    "                                self.shifts.append((w, action_obj[1]))\n",
    "                            if action_obj[0] == 'r':\n",
    "                                t = action_obj[2]\n",
    "                                if t == 0:\n",
    "                                    self.add_reduction(w, action_obj[1], 0, action_obj[3], self.sppf.epsilon_sppf[0])\n",
    "                                elif (m != 0):\n",
    "                                    self.add_reduction(u, action_obj[1], t, action_obj[3], z)\n",
    "            \n",
    "            if (m != 0):\n",
    "                # Add children\n",
    "                node_seq = list(path[:-1])\n",
    "                node_seq.reverse()\n",
    "                node_seq.append(y)\n",
    "                if (f != 0):\n",
    "                    node_seq.append(self.sppf.epsilon_sppf[f])\n",
    "                \n",
    "                z.add_children(node_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b01162",
   "metadata": {},
   "source": [
    "**Shifter($i$)**\n",
    "- $\\mathcal{Q}' \\gets\\emptyset$\n",
    "- Create SPPF node $z$ with label $(a_i, i)$\n",
    "- While $\\mathcal{Q} \\ne \\emptyset$\n",
    "\t- Pop $(v, k)$ at the top of $\\mathcal{Q}$\n",
    "\t- If exists node $w$ with label $k$ in $U_i$\n",
    "\t\t- Create edge $(w, v)$ with label $z$\n",
    "\t\t- For $r(B, t, f)\\in T(k, a_{i+1})$\n",
    "\t\t\t- If $t\\ne 0$ add $(v, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t- Else\n",
    "\t\t- Create node $w$ with label $k$ in $U_i$\n",
    "\t\t- Create edge $(w, v)$ with label $z$\n",
    "\t\t- For action in $T(k, a_{i + 1})$\n",
    "\t\t\t- If shift action $ph$\n",
    "\t\t\t\t- Add $(w, h)$ to $\\mathcal{Q'}$\n",
    "\t\t\t- If reduce action $r(B, t, f)$\n",
    "\t\t\t\t- If $t=0$ add $(w, B, 0, f, \\epsilon)$ to $\\mathcal{R}$\n",
    "\t\t\t\t- if $t\\ne 0$ add $(v, B, t, f, z)$ to $\\mathcal{R}$\n",
    "- $\\mathcal{Q} \\gets\\mathcal{Q'}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b06c3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNGLRParser(RNGLRParser):\n",
    "    def shifter(self, i: int):\n",
    "        '''\n",
    "            The shifter, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "        '''\n",
    "        new_q: list[tuple[GSSNode, int]] = []\n",
    "        z = self.sppf.create_node(self.input_str[i], i)\n",
    "\n",
    "        while (len(self.shifts) > 0):\n",
    "            v, k = self.shifts.pop()\n",
    "            # print(f\"[Shifter level ${i}] processing: \", v, k)\n",
    "            node = self.gss.find_node(k, i + 1)\n",
    "            if node is not None:\n",
    "                node.add_child(v, z)\n",
    "                for action in self.table[k][self.input_str[i + 1]]:\n",
    "                    action_obj = get_action(action)\n",
    "                    if action_obj[0] == 'r' and action_obj[2] != 0:\n",
    "                        self.add_reduction(v, action_obj[1], action_obj[2], action_obj[3], z)\n",
    "            else:\n",
    "                new_node = self.gss.create_node(k, i + 1)\n",
    "                new_node.add_child(v, z)\n",
    "\n",
    "                for action in self.table[k][self.input_str[i + 1]]:\n",
    "                    action_obj = get_action(action)\n",
    "                    \n",
    "                    if action_obj[0] == 'p':\n",
    "                        # print(\"    Added shift: \", new_node, action_obj[1])\n",
    "                        new_q.append((new_node, action_obj[1]))\n",
    "                    if action_obj[0] == 'r':\n",
    "                        # Reduce action\n",
    "                        if action_obj[2] == 0:\n",
    "                            self.add_reduction(new_node, action_obj[1], 0, action_obj[3], self.sppf.epsilon_sppf[0])\n",
    "                        else:\n",
    "                            self.add_reduction(v, action_obj[1], action_obj[2], action_obj[3], z)\n",
    "        \n",
    "        self.shifts = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cb5ab",
   "metadata": {},
   "source": [
    "##### A parse example\n",
    "**Grammar 5.3**\n",
    "![automata](images/grammar_53_auto.png)\n",
    "\n",
    "Parse table:\n",
    "\n",
    "| State | B   | C   | S   | a   | b             | $                     |\n",
    "| ----- | --- | --- | --- | --- | ------------- | --------------------- |\n",
    "| 0     |     |     | p1  | p2  |               |                       |\n",
    "| 1     |     |     |     |     |               | acc                   |\n",
    "| 2     | p4  |     |     |     | p3/r(B, 0, 1) | r(B, 0, 1)/r(S, 1, 3) |\n",
    "| 3     |     |     |     |     | r(B, 1, 0)    | r(B, 1, 0)            |\n",
    "| 4     | p6  |     |     |     | p5            | r(B, 0, 1)/r(S,2,4)   |\n",
    "| 5     |     |     |     |     |               | r(B, 1, 0)            |\n",
    "| 6     |     | p7  |     |     |               | r(C, 0, 2)/r(S, 3, 2) |\n",
    "| 7     |     |     |     |     |               | r(S, 4, 0)            |\n",
    "\n",
    "Let's see how the parser works with the input \"ab\".\n",
    "\n",
    "Firstly, the $\\epsilon$-SPPF tree is constructed as above, and the GSS is initialised with one node $v_0$ that has label $S_0$. We look at $T(0, a)$, which contains only one action $p2$, so we add $(v_0, 2)$ to $\\mathcal{Q}$. The $\\mathrm{Shifter}$ processes $(v_0, 2)$, creates a new SPPF node $w_1$ labelled $(a, 0)$ and a new GSS node $v_1$ with label $S_2$. Edge $(v_0, v_1)$ is then created with label $(a, w_1)$. A new edge is created so we look at $T(2, b)$ and find a shift/reduce conflict, we add $(v_1, 3)$ to $\\mathcal{Q}$ and $(v_1, B, 0, 1, \\epsilon)$ to $\\mathcal{R}$. We finish the construction of GSS level 1.\n",
    "![parse_1](images/parse_example_1.png)\n",
    "In the next iteration, $\\textrm{Reducer}$ finds $(v_1, B, 0, 1, \\epsilon)$ on top, the length of reduction is $m=0$ so the only possible path is $\\{v_1\\}$. The $\\textrm{Reducer}$ then creates a new $v_2$ node in GSS in the same level of $v_1$, node $v_2$ has label $T(2, B) = S_4$, with an edge pointing to $v_1$. We label the edge by $(B, u_1)$, here $u_1$ is the $\\epsilon$-SPPF node of the non-terminal $B$.\n",
    "![parse_2](images/parse_example_2.png)\n",
    "\n",
    "In $T(4, b)$ there is a shift action, so we add $(v_2, 5)$ to $\\mathcal{Q}$. Currently there are 2 shift actions in $Q$: $(v_1, 3)$ and $(v_2, 5)$. We process those shift actions and create 2 new nodes in GSS ($v_3$ and $v_4$) along with one new $w_2$ node in SPPF. Checking $T(3, \\$)$ and $T(5, \\$)$, we find a reduction $r(B, 1, 0)$, so $(v_1, B, 1, 0, w_2)$ and $(v_2, B, 1, 0, w_2)$ are added to $\\mathcal{R}$.\n",
    "![parse_3](images/parse_example_3.png)\n",
    "Now, the $\\textrm{Reducer}$ is called once more. It gets the top element $(v_1, B, 1, 0, w_2)$, then searches for the path with length $m-1=0$, so the only path is $\\{v_1\\}$. Since $m\\ne0$, in addition to new GSS node $v_5$ (label $S_4$), we also create a new SPPF node $w_3$ with label $(B, 1)$. Edge $(v_5, v_1)$ is labelled with $B, w_3$. In $T(4, \\$)$ there is a reduce/reduce conflict between $r(B, 0, 1)$ and $r(S, 2, 4)$, so we add the newly found reductions $(v_5, B, 0, 1, \\epsilon)$ and $(v_1, S, 2, 4, w_3)$ to $\\mathcal{R}$. Finally, we add $w_2$ as node $w_3$'s child.\n",
    "![parse_4](images/parse_example_4.png)\n",
    "The next reduction is $(v_2, B, 1, 0, w_2)$. Since $m=1 \\ne0$, we try to create SPPF node $(B, 1)$ but it already exists. In the GSS, a new node $v_6$ with label $S_6$ is created along with a new edge $(v_6, v_2)$, this new edge has label $B, w_3$. In $T(6, \\$)$ there is also a reduce/reduce conflict between $r(C, 0, 2)$ and $r(S, 3, 2)$. We add both $(v_6, C, 0, 2, \\epsilon)$ and $(v_2, S, 3, 2, w_3)$ to $\\mathcal{R}$.\n",
    "![parse_5](images/parse_example_5.png)\n",
    "By now we have $\\mathcal{R} = \\{(v_5, B, 0, 1, \\epsilon), (v_1, S, 2, 4, w_3), (v_6, C, 0, 2, \\epsilon), (v_2, S, 3, 2, w_3)\\}$. Process the first reduction, $v_5$ has label $S_4$, and $T(4, B) = p6$ which is the same as label in $v_6$, so new edge $(v_6, v_5)$ is added with label $B, u_1$. We don't add new reduction here because the reduction length is zero.\n",
    "\n",
    "Next is $(v_1, S, 2, 3, w_3)$, we find all paths that start from $v_1$ and have length of one. In this case, the only path is $\\{v_1, v_0\\}$, along the path we also collect the SPPF node $w_1$ on edge $(v_1, v_0)$. We create new GSS node $v_7$ with label $S_2$ and edge $(v_7, v_0)$. We also create a new SPPF node $w_4$ (label $S, 0$) with 3 children: $w_1$ (collected from the path), $w_3$ (from the top element) and $u_f=u_4$. We add $w_4$ to $\\mathcal{N}$.\n",
    "![parse_7](images/parse_example_7.png)\n",
    "Processing the reduction $(v_6, C, 0, 2, \\epsilon)$, we create GSS node $v_8$, label $S_7$, with edge $(v_8, v_6)$. The new edge is labelled with $C, u_2$. No further reduction is added.\n",
    "\n",
    "Finally, we process $(v_2, S, 3, 2, w_3)$. We search for the paths with length 2 starting from $v_2$, the only possible path is $\\{v_2, v_1, v_0\\}$ and we collect $u_1$ and $w_1$ along the way. The corresponding SPPF node is $(S, 0)$, which already exists the the set $\\mathcal{N}$ as $w_4$. The sequence of children is $[w_1, u_1, w_3, u_2]$, we append this sequence into $w_4$ by creating two new packing nodes.\n",
    "![parse_8](images/parse_example_8.png)\n",
    "By now we have no reduction left and the parse is finished. At GSS level 2, $S_2$ is the accept state so we accept the string \"ab\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6dd0e-c684-49e7-95e9-017a34ab6a7e",
   "metadata": {},
   "source": [
    "Let us parse the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "083e819d-58d9-440e-9bd1-501e9c6719a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input_str = \"ab\"\n",
    "parser = RNGLRParser(start_symbol, grammar_53, table)\n",
    "result, sppf_root = parser.parse(input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284452e-bb11-4ce6-8297-b0d3b8d94cad",
   "metadata": {},
   "source": [
    "The string is accepted, that is a good sign. Now we look at the internal GSS and SPPF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d679b438-1cf1-47d8-88af-4708760e0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSS:\n",
      "Level 0:\n",
      "    Node(v0, 0)\n",
      "Level 1:\n",
      "    Node(v1, 2)\n",
      "        Node(v0, 0) - SPPF Node:(a, 0)\n",
      "    Node(v2, 4)\n",
      "        Node(v1, 2) - SPPF Node:(<B>, -1)\n",
      "Level 2:\n",
      "    Node(v3, 5)\n",
      "        Node(v2, 4) - SPPF Node:(b, 1)\n",
      "    Node(v4, 3)\n",
      "        Node(v1, 2) - SPPF Node:(b, 1)\n",
      "    Node(v5, 4)\n",
      "        Node(v1, 2) - SPPF Node:(<B>, 1)\n",
      "    Node(v6, 6)\n",
      "        Node(v5, 4) - SPPF Node:(<B>, -1)\n",
      "        Node(v2, 4) - SPPF Node:(<B>, 1)\n",
      "    Node(v7, 7)\n",
      "        Node(v6, 6) - SPPF Node:(<C>, -1)\n",
      "    Node(v8, 1)\n",
      "        Node(v0, 0) - SPPF Node:(<S>, 0)\n",
      "\n",
      "SPPF:\n",
      "    a-0\n",
      "    b-1\n",
      "    <B>-1\n",
      "        SPPF Node:(b, 1)\n",
      "    <S>-0\n",
      "        PackingNode\n",
      "            SPPF Node:(a, 0)\n",
      "            SPPF Node:(<B>, 1)\n",
      "            SPPF Node:(<B><C>, -1)\n",
      "        PackingNode\n",
      "            SPPF Node:(a, 0)\n",
      "            SPPF Node:(<B>, -1)\n",
      "            SPPF Node:(<B>, 1)\n",
      "            SPPF Node:(<C>, -1)\n",
      "    €-2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser.gss)\n",
    "print(parser.sppf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1c80b-0ee6-4d7e-aa6e-767db0a21ef6",
   "metadata": {},
   "source": [
    "In this output, nodes are listed one by one. Their direct children are then listed on subsequent lines, indented by one level. For example, SPPF node `<S>-0` has two packing nodes as children, and the packing nodes also have their own children. SPPF nodes with position -1 is from the $\\epsilon$-SPPF tree. The structure is the same as the example, the parse is successful. To visualise the parse tree, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "561ea39f-3c35-4149-b846-d9eb13bf4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>\n",
      "├─ 'a'\n",
      "├─ <B>\n",
      "│   └─ 'b'\n",
      "└─ <B><C>\n",
      "    ├─ <B>\n",
      "    │   └─ 'epsilon'\n",
      "    └─ <C>\n",
      "        └─ 'epsilon'\n",
      "<S>\n",
      "├─ 'a'\n",
      "├─ <B>\n",
      "│   └─ 'epsilon'\n",
      "├─ <B>\n",
      "│   └─ 'b'\n",
      "└─ <C>\n",
      "    └─ 'epsilon'\n"
     ]
    }
   ],
   "source": [
    "ee = EnhancedExtractor(sppf_root)\n",
    "while True:\n",
    "    t = ee.extract_a_tree()\n",
    "    if t is None: break\n",
    "    display_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cc402e-4d30-4d95-8607-ed4f70a9d8c1",
   "metadata": {},
   "source": [
    "We can see that there are 2 possible parse trees in this example.\n",
    "\n",
    "#### Time complexity of RNGLR\n",
    "Even though RNGLR can parse in an LR-like manner, in the worst case its time complexity is non-polynomial, $O(n^{M+1})$, where $M$ is the largest length of a reduction. The bottleneck comes from our path-searching function in GSS, because there can be an exponential number of paths. This is somewhat disappointing, since other general parsers like Earley or CYK display $O(n^3)$ time complexity in the worst case.\n",
    "\n",
    "To improve this weakness of RNGLR, a simple approach is to refactor the grammar, eliminating any rule with length more than 2 (for example, transforming into Chomsky Normal Form). However, doing so can introduce many side effects, and increase the parse table size, as well as difficulty in building the parse tree. A more promising solution is BRNGLR; by limiting the reduction length to 2 via some smart optimisations, we don't have to perform extensive searching, hence improving the worst case to $O(n^3)$, on par with other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b3f56-5dbc-412b-a7bd-bad375b6ae1c",
   "metadata": {},
   "source": [
    "## Binary Right-Nulled GLR (BRNGLR)\n",
    "\n",
    "The main idea of BRNGLR is to perform reduction of with length less than or equal to 2 only. Consider the following rule $S \\rightarrow ABC$. We would first reduce $S\\rightarrow AS_1$ and then $S_1\\rightarrow BC$. However we don't have to modify the grammar or the parse table. The reduction can be processed \"on-the-fly.\"\n",
    "\n",
    "When the parser processes an element of the form $(w, X, m, f, z)$ in $\\mathcal{R}$, if $m > 2$, it creates a bookkeeping node called $X_m$ in the current level. Next, all elements $(u, X, m-1, f, z)$ are added back into $\\mathcal{R}$ (where $u$ is the child of $v$). This approach ensures that reductions of length $m > 2$ are done in $m-1$ steps. The bookkeeping node is to prevent redundant path searching in later steps.\n",
    "\n",
    "#### Parsing example\n",
    "This time, we will use the following grammar\n",
    "$$\\begin{split}S&\\rightarrow a\\ b\\ c\\ d\\ |\\ a\\ b\\ c\\ D\\\\\n",
    "D &\\rightarrow d\\end{split}$$\n",
    "Since the longest reduction is $S\\rightarrow a\\ b\\ c\\ D$ has 4 symbols, let's call it `grammar_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8a26537-c14f-479f-8dbd-aad040367e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_4 =  {\n",
    "    \"<S>\": [[\"a\", \"b\", \"c\", \"d\"], [\"a\", \"b\", \"c\", \"<D>\"]],\n",
    "    \"<D>\": [[\"d\"]]\n",
    "}\n",
    "start_4 = \"<S>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde61acf-5cd8-42a2-9926-7d21a4a7bfbc",
   "metadata": {},
   "source": [
    "The parse table is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d180fdcd-fb4f-4c39-b74c-c411e2308b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0\n",
      "<S> ->  · a b c <D>, €\n",
      "<S'> ->  · <S>, €\n",
      "<S> ->  · a b c d, €\n",
      "\n",
      "State 1\n",
      "<S'> -> <S> · , €\n",
      "\n",
      "State 2\n",
      "<S> -> a · b c <D>, €\n",
      "<S> -> a · b c d, €\n",
      "\n",
      "State 3\n",
      "<S> -> a b · c <D>, €\n",
      "<S> -> a b · c d, €\n",
      "\n",
      "State 4\n",
      "<S> -> a b c · <D>, €\n",
      "<S> -> a b c · d, €\n",
      "<D> ->  · d, €\n",
      "\n",
      "State 5\n",
      "<S> -> a b c d · , €\n",
      "<D> -> d · , €\n",
      "\n",
      "State 6\n",
      "<S> -> a b c <D> · , €\n",
      "\n",
      "---------------\n",
      "Transition map\n",
      "GOTO (0, '<S>') = 1\n",
      "GOTO (0, 'a') = 2\n",
      "GOTO (2, 'b') = 3\n",
      "GOTO (3, 'c') = 4\n",
      "GOTO (4, 'd') = 5\n",
      "GOTO (4, '<D>') = 6\n",
      "\n",
      "Parsing table:\n",
      "\n",
      "     <D>         <S>         a           b           c           d           €            \n",
      "\n",
      "I0               p1          p2                                                          \n",
      "I1                                                                           acc         \n",
      "I2                                       p3                                              \n",
      "I3                                                   p4                                  \n",
      "I4   p6                                                          p5                      \n",
      "I5                                                                           r<S>4.0/r<D>1.0\n",
      "I6                                                                           r<S>4.0     \n"
     ]
    }
   ],
   "source": [
    "generator_4 = TableGenerator(grammar_4, start_4)\n",
    "table_4 = generator_4.generate_parse_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f12e6-b932-44ba-b7cb-875501f04238",
   "metadata": {},
   "source": [
    "In diagram form:\n",
    "![grammar_4_automaton](images/grammar_4_automaton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0fc1b-e1e7-4f0a-9f0c-6d46ebaffe70",
   "metadata": {},
   "source": [
    "The input string is \"abcd\". Firstly, we also create the node $v_0$ with label 0 in the GSS. The only action in $T(0, a)$ is $p2$, so we add $(v_0, 2)$ to $\\mathcal{Q}$. The $\\textrm{Shifter}$ then processes this, creating a new SPPF node $w_1$ labelled $(a, 0)$. Repeating this shifting procedure for the next three symbols $b, c$, and $d$ gives us\n",
    "![parse_4_1](images/parse_4_example_1.png)\n",
    "\n",
    "At this point there is a reduce/reduce conflict in $T(5, \\$)$ between $r(D, 1, 0)$ and $r(S, 4, 0)$, so we add in $\\mathcal{R}$ there is $\\{(v_3, S, 4, 0, w_4), (v_3, D, 1, 0, w_4)\\}$. We process $(v_3, S, 4, 0, w_4)$, the reduction length $m = 4 > 2$ so we create a new bookkeeping node $v_5$ with label $S_4$ in the GSS. Since $v2$ is the only child of $v_3$, we connect the new $v_5$ node to $v_2$. In the SPPF, we also make a new $w_5$ node, this node has empty label, and its children are $w_4$ (from the item) and $w_3$ (collected on the path $v_3 \\rightarrow v_2$). Finally, we label edge $(v_5, v_2)$ with $w_5$ and add $(v_2, S, 3, 0, w_5)$ back into $\\mathcal{R}$.\n",
    "![parse_4_2](images/parse_4_example_2.png)\n",
    "The next reduce action is $(v_3, D, 1, 0, w_4)$. Here, the reduction length is 1, which is less than 2 so the $\\textrm{Reducer}$ processes as usual. It creates a new GSS node $v_6$ (label 6) that links back to $v_3$, and then adds a new SPPF node $w_6$ with label $(D, 3)$, which only has one child $w_4$. In $T(6, \\$)$ there is a reduction $r(S, 4, 0)$ so we add $(v_3, S, 4, 0, w_6)$ to $\\mathcal{R}$.\n",
    "![parse_4_3](images/parse_4_example_3.png)\n",
    "Processing $(v_2, S, 3, 0, w_5)$, because the length is 3 larger than 2, we create new GSS node $v_7$ with label $S_3$ and connect it to $v_1$ - the only child of $v_2$. In the SPPF, we also create a new blank node $w_7$ with $w_5$ and $w_2$ as children. Finally we add the reduction $(v_1, S, 2, 0, w_7)$ to $\\mathcal{R}$.\n",
    "![parse_4_4](images/parse_4_example_4.png)\n",
    "Now $(v_3, S, 4, 0, w_6)$ is the top reduction. There is already an $S_4$ GSS node in this level so we don't have to create a new one. However, we still update the SPPF node $w_5$ to reflect this reduction. Node $w_5$ now have another sequence of children: $[w_3, w_6]$, we create two packing nodes to store them. \n",
    "![parse_4_5](images/parse_4_example_5.png)\n",
    "The only reduction left is $(v_1, S, 2, 0, w_7)$. The length of reduction is 2 so we create a new GSS node $v_8$ with label 1, and connect it to $v_0$-the only child of $v_1$. In the SPPF, node $w_8$ is created with label $(S, 0)$ and its children is $[w_1, w_7]$.\n",
    "![parse_4_6](images/parse_4_example_6.png)\n",
    "Now the parse is complete, we accept the string \"abcd\" because the accept state 1 is present in level 4 of the GSS, and the SPPF is successfully built as showed.\n",
    "\n",
    "The initial reduction $(v_3, S, 4, 0, w_4)$ was performed in three steps: $(v_3, S, 4, 0, w_4)$, $(v_2, S, 3, 0, w_5)$, and $(v_1, S, 2, 0, w_7)$. This approach helps us minimise the necessary path searching in GSS. We can observe this benefit when processing the reduction $(v_3, S, 4, 0, w_6)$, only SPPF tree was updated. Although the GSS size is larger in BRNGLR, it grows in a constant rate, making it overall more efficient than RNGLR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafe460-2065-4110-be24-bea2fe919930",
   "metadata": {},
   "source": [
    "#### Pseudo code and implementation\n",
    "First we define the BRNGLR class with its helper methods. For the most part this is similar to RNGLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99b23ae8-448c-4ff2-9f4c-f6b9648a4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNGLRParser(RNGLRParser):\n",
    "    '''\n",
    "        The BRNGLR parser\n",
    "    '''\n",
    "    def __init__(self, start: str, grammar: Grammar, table: dict[int, dict[str, list[str]]]):\n",
    "        super().__init__(start, grammar, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e763ee-06b3-458a-870d-6b9f1fe4fc43",
   "metadata": {},
   "source": [
    "##### Parse function\n",
    "The $\\textrm{Parse}$ function is exactly the same as RNGLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b89a00e8-5935-420b-9dae-af2d75fb75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNGLRParser(BRNGLRParser):\n",
    "    def parse(self, input_str: str):\n",
    "        # Same as RNGLR\n",
    "        return super().parse(input_str)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1c2e5-32bb-47b0-be0e-a11afa1ad906",
   "metadata": {},
   "source": [
    "##### The Reducer\n",
    "**Pseudocode**\n",
    "\n",
    "Get $(v, X, m, f, y)$ from $\\mathcal{R}$\n",
    "- If $m\\ge 2$\n",
    "\t- $\\mathcal{X}$ is the set of $(u, x)$ where $u$ is child of $v$ and $x$ is the label of edge $(v, u)$\n",
    "- Else $\\mathcal{X} = \\{(v, \\epsilon)\\}$\n",
    "- If $m\\le 2$\n",
    "\t- For every $(u, x) \\in \\mathcal{X}$\n",
    "\t\t- If $m = 0$\n",
    "\t\t\t- $z\\gets$ node $f$ in the $\\epsilon$-SPPF tree\n",
    "\t\t- Else\n",
    "\t\t\t- Let $c$ be the level of $u$ in GSS\n",
    "\t\t\t- Find SPPF node $z = (X, c)$ in $\\mathcal{N}$\n",
    "\t\t\t\t- If does not exist then create $z$ and add to $\\mathcal{N}$\n",
    "\t\t- Let $k$ be the label of $u$ and $pl$ be the shift action in $T(k, a_i)$\n",
    "\t\t- If exists node $w$ with label $l$ in $U_i$\n",
    "\t\t\t- If edge $(w, u)$ does not exist\n",
    "\t\t\t\t- Create edge $(w, u)$ with label $z$\n",
    "\t\t\t\t- For $r(B, t, f) \\in T(l, a_i)$\n",
    "\t\t\t\t\t- If $t \\neq 0$ add $(u, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t\t- Else\n",
    "\t\t\t- Create node $w$\n",
    "\t\t\t- Create edge $(w, u)$ with label $z$\n",
    "\t\t\t- For action in $T(l, a_i)$\n",
    "\t\t\t\t- If shift action $ph$\n",
    "\t\t\t\t\t- Add $(w, h)$ to $\\mathcal{Q}$\n",
    "\t\t\t\t- If reduce action $r(B, t, f)$\n",
    "\t\t\t\t\t- If $t = 0$\n",
    "\t\t\t\t\t\t- Add $(w, B, t, f, \\epsilon)$ to $\\mathcal{R}$\n",
    "\t\t\t\t\t- if $t\\ne 0$ and $m\\neq 0$\n",
    "\t\t\t\t\t\t- Add $(w, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t\t\t- If $m = 1$\n",
    "\t\t\t\t- $nodeSequence = [y]$\n",
    "\t\t\t- if $m = 2$\n",
    "\t\t\t\t- $nodeSequence = [x, y]$\n",
    "\t\t\t- if $f\\ne 0$\n",
    "\t\t\t\t- Append $u_f$ to $nodeSequence$\n",
    "\t\t\t- If $m\\ne 0$\n",
    "\t\t\t\t- $z.\\textrm{addChildren}(nodeSequence)$\n",
    "- Else $(m > 2)$\n",
    "    - If node $w$ with label $X_m$ doesn't exist\n",
    "        - Create one\n",
    "    - For $(u, x)\\in \\mathcal{X}$\n",
    "        - If there isn't edge $(w, u)$\n",
    "            - Create empty SPPF node $z$\n",
    "            - Create edge $(w, u)$\n",
    "            - Add $(u, X, m -1, 0, z)$ to $\\mathcal{R}$\n",
    "        - $nodeSequence = [x, y]$\n",
    "        - If $f\\ne 0$:\n",
    "            - Append $u_f$ to $nodeSequence$\n",
    "        - $z.\\textrm{addChildren}(nodeSequence)$\n",
    "\n",
    "Here we call $u_f$ the $\\epsilon$-SPPF node with index $f$. The main reducer logic is divided into 2 cases, $m \\le 2$ and $m > 2$. \n",
    "\n",
    "In the first case $m \\le 2$, we don't have to perform a search on GSS anymore because the only possible lengths of paths are 0 and 1, either direct children or the node itself. Other than that, the reducer behaves similarly to RNGLR in this case.\n",
    "\n",
    "In the second case, the $\\textrm{Reducer}$ creates a new bookkeeping GSS node and an empty SPPF node. It then adds a new reduction with reduced length into $\\mathcal{R}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "affa8405-2ef1-43ef-b6ba-2cfc6227778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNGLRParser(BRNGLRParser):\n",
    "    def reducer(self, i: int):\n",
    "        '''\n",
    "            The reducer, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "        '''\n",
    "        v, X, m, f, y = self.reductions.pop()\n",
    "\n",
    "        X_: list[tuple[GSSNode, SPPFNode]] = []\n",
    "        z: SPPFNode = None\n",
    "        if (m >= 2):\n",
    "            for child in v.children:\n",
    "                X_.append(child)\n",
    "        else:\n",
    "            X_ = [(v, self.sppf.epsilon_sppf[0])]\n",
    "        \n",
    "        if (m <= 2):\n",
    "            for u, x in X_:\n",
    "                k = u.label\n",
    "\n",
    "                if m == 0:\n",
    "                    z = self.sppf.epsilon_sppf[f]\n",
    "                else:\n",
    "                    c = u.level\n",
    "                    if (X, c) not in self.set_N:\n",
    "                        z = self.sppf.create_node(X, c)\n",
    "                        self.set_N[X, c] = z\n",
    "                    else:\n",
    "                        z = self.set_N[(X, c)]\n",
    "                \n",
    "                for action in self.table[k][X]:\n",
    "                    action_obj = get_action(action)\n",
    "                    if action_obj[0] == 'p':\n",
    "                        w = self.gss.find_node(action_obj[1], i)\n",
    "                        if w is not None:\n",
    "                            if u not in [x[0] for x in w.children]:\n",
    "                                w.add_child(u, z)\n",
    "                                if m != 0:\n",
    "                                    for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                                        action_obj = get_action(action)\n",
    "                                        if action_obj[0] == 'r' and action_obj[2] != 0:\n",
    "                                            # (u, B, t, f, z)\n",
    "                                            self.reductions.append((u, action_obj[1], action_obj[2], action_obj[3], z))\n",
    "                        else:\n",
    "                            w = self.gss.create_node(action_obj[1], i)\n",
    "                            w.add_child(u, z)\n",
    "                            for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                                action_obj = get_action(action)\n",
    "                                if action_obj[0] == 'p':\n",
    "                                    self.shifts.append((w, action_obj[1]))\n",
    "                                if action_obj[0] == 'r':\n",
    "                                    t = action_obj[2]\n",
    "                                    if t == 0:\n",
    "                                        self.reductions.append((w, action_obj[1], 0, action_obj[3], self.sppf.epsilon_sppf[0]))\n",
    "                                    elif (m != 0):\n",
    "                                        self.reductions.append((u, action_obj[1], t, action_obj[3], z))\n",
    "                \n",
    "                node_seq: list[SPPFNode] = []\n",
    "                if (m == 1):\n",
    "                    node_seq = [y]\n",
    "                elif (m == 2):\n",
    "                    node_seq = [x, y]\n",
    "                if f != 0:\n",
    "                    node_seq.append(self.sppf.epsilon_sppf[f])\n",
    "                \n",
    "                if m != 0:\n",
    "                    z.add_children(node_seq)\n",
    "        else:\n",
    "            w = self.gss.find_node(f\"{X}_{m}\", i)\n",
    "            if w is None:\n",
    "                w = self.gss.create_node(f\"{X}_{m}\", i)\n",
    "            \n",
    "            for u, x in X_:\n",
    "                z: SPPFNode = None\n",
    "    \n",
    "                for child, edge in w.children:\n",
    "                    if child == u:\n",
    "                        z = edge\n",
    "                        break\n",
    "                if z is None:\n",
    "                    z = self.sppf.create_node(\"\", u.level)\n",
    "                    w.add_child(u, z)\n",
    "                    self.add_reduction(u, X, m - 1, 0, z)\n",
    "                \n",
    "                node_seq: list[SPPFNode] = [x, y]\n",
    "                if f != 0:\n",
    "                    node_seq.append(self.sppf.epsilon_sppf[f])\n",
    "                z.add_children(node_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2518d8c-5c58-46d1-a7f8-4b215f050d85",
   "metadata": {},
   "source": [
    "##### The shifter\n",
    "We keep the same RNGLR logic for the shifter, there is no changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1cec519-6848-4a99-94dd-fa1c55ad7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNGLRParser(BRNGLRParser):\n",
    "    def shifter(self, i: int):\n",
    "        # Same as RNGLR\n",
    "        super().shifter(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1becc-8549-4fd1-9d09-7d10096eb066",
   "metadata": {},
   "source": [
    "Now we can use the new BRNGLR parser to parse the string \"abcd\" in `grammar_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1eba703-d950-461c-944c-d5bd77e9043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input_str_4 = \"abcd\"\n",
    "parser_4 = BRNGLRParser(start_4, grammar_4, table_4)\n",
    "result_4, sppf_root_4 = parser_4.parse(input_str_4)\n",
    "print(result_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04607f3a-d386-4b23-8308-4ca2a7dc1d93",
   "metadata": {},
   "source": [
    "The parser accepted it, let's look at the GSS and SPPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7916f837-2b01-4048-8ca3-f8b370aa1b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSS:\n",
      "Level 0:\n",
      "    Node(v0, 0)\n",
      "Level 1:\n",
      "    Node(v1, 2)\n",
      "        Node(v0, 0) - SPPF Node:(a, 0)\n",
      "Level 2:\n",
      "    Node(v2, 3)\n",
      "        Node(v1, 2) - SPPF Node:(b, 1)\n",
      "Level 3:\n",
      "    Node(v3, 4)\n",
      "        Node(v2, 3) - SPPF Node:(c, 2)\n",
      "Level 4:\n",
      "    Node(v4, 5)\n",
      "        Node(v3, 4) - SPPF Node:(d, 3)\n",
      "    Node(v5, 6)\n",
      "        Node(v3, 4) - SPPF Node:(<D>, 3)\n",
      "    Node(v6, <S>_4)\n",
      "        Node(v2, 3) - SPPF Node:(blank)\n",
      "    Node(v7, <S>_3)\n",
      "        Node(v1, 2) - SPPF Node:(blank)\n",
      "    Node(v8, 1)\n",
      "        Node(v0, 0) - SPPF Node:(<S>, 0)\n",
      "\n",
      "SPPF:\n",
      "    a-0\n",
      "    b-1\n",
      "    c-2\n",
      "    d-3\n",
      "    <D>-3\n",
      "        SPPF Node:(d, 3)\n",
      "    blank\n",
      "        PackingNode\n",
      "            SPPF Node:(c, 2)\n",
      "            SPPF Node:(<D>, 3)\n",
      "        PackingNode\n",
      "            SPPF Node:(c, 2)\n",
      "            SPPF Node:(d, 3)\n",
      "    blank\n",
      "        SPPF Node:(b, 1)\n",
      "        SPPF Node:(blank)\n",
      "    <S>-0\n",
      "        SPPF Node:(a, 0)\n",
      "        SPPF Node:(blank)\n",
      "    €-4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parser_4.gss)\n",
    "print(parser_4.sppf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db27541-d52e-4477-bd35-d0445a73d007",
   "metadata": {},
   "source": [
    "We can also visualise the parse tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dea2f36-60c1-49f2-b205-e93d410fa4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>\n",
      "├─ 'a'\n",
      "└─ ''\n",
      "    ├─ 'b'\n",
      "    └─ ''\n",
      "        ├─ 'c'\n",
      "        └─ <D>\n",
      "            └─ 'd'\n",
      "<S>\n",
      "├─ 'a'\n",
      "└─ ''\n",
      "    ├─ 'b'\n",
      "    └─ ''\n",
      "        ├─ 'c'\n",
      "        └─ 'd'\n"
     ]
    }
   ],
   "source": [
    "ee = EnhancedExtractor(sppf_root_4)\n",
    "while True:\n",
    "    t = ee.extract_a_tree()\n",
    "    if t is None: break\n",
    "    display_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36bd84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### References\n",
    "[1] D. E. Knuth, “On the translation of languages from left to right,” _Information and Control_, vol. 8, no. 6, pp. 607–639, Dec. 1965, doi: [10.1016/S0019-9958(65)90426-2](https://doi.org/10.1016/S0019-9958\\(65\\)90426-2).\n",
    "\n",
    "[2] G. R. Economopoulos, “Generalised LR parsing algorithms”. Retrieved from https://core.ac.uk/download/pdf/301667613.pdf\n",
    "\n",
    "[3] M. Tomita, _Efficient Parsing for Natural Language_. Boston, MA: Springer US, 1986. doi: \n",
    "[10.1007/978-1-4757-1885-0](https://doi.org/10.1007/978-1-4757-1885-0)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
